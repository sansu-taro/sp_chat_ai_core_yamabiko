from .chat_engine_adk_bq import AdkChatbot
from google import genai
from google.genai import types


from typing import TypedDict, Annotated, List, Sequence
import operator
import json
from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
import logging

# ----------------------------------------------------------------
# 1. Stateã®å®šç¾© (æ‹¡å¼µæ¸ˆã¿)
# ----------------------------------------------------------------
class AgentState(TypedDict):
    session_id: str
    message_index: int
    user_question: str
    retrieved_context: str
    human_readable_context: str
    initial_answer: str
    fact_check_result: dict
    final_answer: str
    route_decision: str
    messages: Sequence[BaseMessage]

    # â–¼â–¼â–¼ æ¤œç´¢ãƒ«ãƒ¼ãƒ—ç®¡ç†ç”¨ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ â–¼â–¼â–¼
    current_query: str          # ç¾åœ¨ã®æ¤œç´¢ã‚¯ã‚¨ãƒª
    search_attempts: int        # æ¤œç´¢ã®è©¦è¡Œå›æ•°
    sufficiency_decision: str   # å›ç­”ãŒååˆ†ã‹ã©ã†ã‹ã®åˆ¤æ–­ ("sufficient" or "insufficient")


# ----------------------------------------------------------------
# 2. Agentã‚¯ãƒ©ã‚¹ï¼ˆæ–°SDKå¯¾å¿œãƒ»æœ€å°å·®åˆ†ï¼‰
# ----------------------------------------------------------------
class SupportOperationAgent:
    def __init__(self, chatbot: AdkChatbot):
        self.chatbot = chatbot
        # AdkChatbotå´ã§æ–°SDKã® Client ã‚’æ—¢ã«ä½œã£ã¦ã„ã‚‹ãªã‚‰æµç”¨ï¼ˆself.chatbot.gclientï¼‰
        # æœªæä¾›ç’°å¢ƒã§ã‚‚å‹•ãã‚ˆã†ã€è‡ªå‰ã§ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ
        self.client = getattr(chatbot, "gclient", None) or genai.Client()
        self.model_name = "gemini-2.5-flash"
        self.logger = logging.getLogger(__name__)

    # --- å…±é€šãƒ˜ãƒ«ãƒ‘ï¼šæ–°SDKã§ã®ç”Ÿæˆå‘¼ã³å‡ºã—ã‚’1ã‹æ‰€ã«é›†ç´„ ---
    def _gen(self, prompt: str, *, response_mime_type: str | None = None, temperature: float | None = 0.0):
        cfg = types.GenerateContentConfig(
            response_mime_type=response_mime_type,
            temperature=0.0 if temperature is None else temperature,
        )
        return self.client.models.generate_content(
            model=self.model_name,
            contents=prompt,
            config=cfg,
        )

    # Node 0: ãƒ«ãƒ¼ã‚¿ãƒ¼ (ä¿®æ­£æ¸ˆã¿)
    def route_query(self, state: AgentState):
        self.logger.info("---ğŸš¦ Node: route_query---")
        user_question = state['messages'][-1].content

        prompt = f"""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ä»¥ä¸‹ã®è³ªå•ãŒã€SmartHRã®è£½å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã«é–¢ã™ã‚‹å…·ä½“çš„ãªæƒ…å ±ï¼ˆæ“ä½œæ–¹æ³•ã€ä»•æ§˜ã€æ–™é‡‘ãªã©ï¼‰ã‚’æ±‚ã‚ã‚‹ã‚‚ã®ã‹ã€
        ãã‚Œã¨ã‚‚ä¸€èˆ¬çš„ãªæŒ¨æ‹¶ã€ãŠç¤¼ã€è‡ªå·±ç´¹ä»‹ãªã©ã®ä¼šè©±ã§ã‚ã‚‹ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚
        - å…·ä½“çš„ãªæƒ…å ±ã‚’æ±‚ã‚ã¦ã„ã‚‹å ´åˆ: "retrieval"
        - ä¸€èˆ¬çš„ãªä¼šè©±ã§ã‚ã‚‹å ´åˆ: "conversational"
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: "{user_question}"
        åˆ¤æ–­çµæœ:
        """
        response = self._gen(prompt)
        route = (getattr(response, "text", "") or "").strip().lower()

        self.logger.info(f"   - åˆ¤æ–­: {route}")
        route_decision = "retrieval" if "retrieval" in route else "conversational"

        # æ¤œç´¢ãƒ«ãƒ¼ãƒ—ã®çŠ¶æ…‹ã‚’åˆæœŸåŒ–ã—ã¦è¿”ã™
        return {
            "user_question": user_question,
            "route_decision": route_decision,
            "current_query": user_question,  # æœ€åˆã®ã‚¯ã‚¨ãƒªã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            "search_attempts": 0,
            "retrieved_context": "",         # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç©ºã§åˆæœŸåŒ–
        }

    # Node 1: æƒ…å ±æ¤œç´¢ (ä¿®æ­£æ¸ˆã¿)
    def retrieve(self, state: AgentState):
        self.logger.info("---ğŸ” Node: retrieve---")
        query = state['current_query']
        session_id = state.get('session_id')
        message_index = state.get('message_index')

        self.logger.info(f"   - æ¤œç´¢ã‚¯ã‚¨ãƒª: \"{query}\"")

        ai_context, human_context = self.chatbot._get_information_for_query(
            query,
            session_id=session_id,
            message_index=message_index,
        )

        # è¤‡æ•°å›ã®æ¤œç´¢çµæœã‚’è¿½è¨˜ã—ã¦ã„ã
        existing_context = state.get('retrieved_context', '')
        updated_context = (existing_context + f"\n\n--- æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã®çµæœ ---\n" + (ai_context or "")).strip()

        existing_human_context = state.get('human_readable_context', '')
        updated_human_context = (existing_human_context + "\n" + (human_context or "")).strip() if existing_human_context else (human_context or "")

        return {
            "retrieved_context": updated_context,
            "human_readable_context": updated_human_context,
        }

    # Node 2: å›ç­”ç”Ÿæˆ
    def generate_initial_answer(self, state: AgentState):
        self.logger.info("---âœï¸ Node: generate_initial_answer (with retrieval)---")
        formatted_history = "".join(
            [f"ãŠå®¢æ§˜: {msg.content}\n" if isinstance(msg, HumanMessage) else f"AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {msg.content}\n"
             for msg in state['messages']]
        )

        prompt = f"""
        ã‚ãªãŸã¯ã€SmartHRã®ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒãƒ¼ãƒ ã«æ‰€å±ã™ã‚‹ã€æ¥µã‚ã¦æ­£ç¢ºã‹ã¤æ…é‡ãªã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
        ã‚ãªãŸã®æœ€å„ªå…ˆäº‹é …ã¯ã€æä¾›ã•ã‚ŒãŸã€Œæ ¹æ‹ æƒ…å ±ã€ã«å®Œå…¨ã«åŸºã¥ã„ãŸã€æ­£ç¢ºç„¡æ¯”ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚

        # å›ç­”ç”Ÿæˆã®å³æ ¼ãªãƒ«ãƒ¼ãƒ«
        ã‚ãªãŸã¯ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’**çµ¶å¯¾çš„ãªé †åº**ã§ã€ä¸€ã¤ãšã¤å®Ÿè¡Œã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚
        1.  **ãƒˆãƒ”ãƒƒã‚¯ã®ä¸€è‡´ç¢ºèª:**
            - ãŠå®¢æ§˜ã®è³ªå•ã«å«ã¾ã‚Œã‚‹ä¸»è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè£½å“åã€æ©Ÿèƒ½åãªã©ï¼‰ã‚’ç‰¹å®šã—ã¾ã™ã€‚
            - ã€Œæ ¹æ‹ æƒ…å ±ã€ã®å„é …ç›®ãŒã€ãã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚
            - **ãŠå®¢æ§˜ã®è³ªå•ã¨ãƒˆãƒ”ãƒƒã‚¯ãŒå…¨ãç•°ãªã‚‹æ ¹æ‹ æƒ…å ±ï¼ˆä¾‹ï¼šè³ªå•ã¯ã€Œæ¡ç”¨ç®¡ç†ã€ãªã®ã«ã€æƒ…å ±ã¯ã€Œéƒ¨ç½²ãƒã‚¹ã‚¿ãƒ¼ã€ï¼‰ã¯ã€å®Œå…¨ã«ç„¡è¦–ã—ã€å›ç­”ã®æ ¹æ‹ ã¨ã—ã¦çµ¶å¯¾ã«ä½¿ç”¨ã—ã¦ã¯ãªã‚Šã¾ã›ã‚“ã€‚**

        2.  **æƒ…å ±æºã®å„ªå…ˆé †ä½ä»˜ã‘:**
            - ã€Œæ ¹æ‹ æƒ…å ±ã€ã«ã¯ã€Œé–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ã€ã¨ã€Œé¡ä¼¼éå»å›ç­”ã€ãŒå«ã¾ã‚Œã¾ã™ã€‚
            - **å¿…ãšã€Œé–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ã€ã®æƒ…å ±ã‚’æœ€å„ªå…ˆ**ã—ã¦ãã ã•ã„ã€‚ã€Œé¡ä¼¼éå»å›ç­”ã€ã¯ã‚ãã¾ã§å‚è€ƒæƒ…å ±ã§ã™ã€‚
            - ã‚‚ã—æƒ…å ±ãŒçŸ›ç›¾ã™ã‚‹å ´åˆï¼ˆä¾‹ï¼šãƒŠãƒ¬ãƒƒã‚¸ã§ã¯ã€Œå¯èƒ½ã€ã€éå»å›ç­”ã§ã¯ã€Œä¸å¯èƒ½ã€ï¼‰ã€**å¿…ãšã€Œé–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ã€ã®å†…å®¹ã‚’æ­£ã¨ã—ã¦æ¡ç”¨**ã—ã¦ãã ã•ã„ã€‚

        3.  **ç›´æ¥çš„ãªæƒ…å ±ã®æ¢ç´¢:**
            - å„ªå…ˆåº¦ã®é«˜ã„ã€Œé–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ã€ã®ä¸­ã«ã€è³ªå•ã«å¯¾ã—ã¦ã€Œã¯ã„ã€å¯èƒ½ã§ã™ã€ã€Œã„ã„ãˆã€ã§ãã¾ã›ã‚“ã€ã®ã‚ˆã†ã«ç›´æ¥çš„ã«å›ç­”ã—ã¦ã„ã‚‹ç®‡æ‰€ãŒãªã„ã‹ã‚’æ¢ã—ã¾ã™ã€‚
            - ã‚‚ã—ç›´æ¥çš„ãªå›ç­”ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ãã‚ŒãŒã‚ãªãŸã®å›ç­”ã®**æ ¸ã¨ãªã‚‹çµè«–**ã§ã™ã€‚ä»–ã®æƒ…å ±ã‹ã‚‰é¡æ¨ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚

        4.  **å¿ å®Ÿãªå›ç­”ã®ç”Ÿæˆ:**
            - ä¸Šè¨˜ã®ãƒ«ãƒ¼ãƒ«ã§ç‰¹å®šã—ãŸã€ä¿¡é ¼ã§ãã‚‹æƒ…å ±**ã®ã¿**ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’ä½œæˆã—ã¾ã™ã€‚
            - **æ ¹æ‹ æƒ…å ±ã«æ›¸ã‹ã‚Œã¦ã„ãªã„äº‹æŸ„ã‚’æ¨æ¸¬ã—ãŸã‚Šã€ç‹¬è‡ªã®è§£é‡ˆã‚’åŠ ãˆãŸã‚Šã™ã‚‹ã“ã¨ã¯å›ºãç¦ã˜ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚**
            - ç‰¹ã«ã€æƒ…å ±ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§å…ƒã®æƒ…å ±ã«ãªã„æ–°ãŸãªçµè«–ï¼ˆä¾‹ï¼šã€Œæ–°è¦ãªã‚‰å¯èƒ½ã§ã€éå»ã¯ä¸å¯èƒ½ã€ãªã©ï¼‰ã‚’**å‰µä½œã—ã¦ã¯ã„ã‘ã¾ã›ã‚“**ã€‚

        5.  **æƒ…å ±ä¸è¶³ã®å ´åˆã®å¯¾å¿œ:**
            - ä¸Šè¨˜ã®æ‰‹é †ã‚’è¸ã‚“ã§ã‚‚ã€è³ªå•ã«ç­”ãˆã‚‰ã‚Œã‚‹ä¿¡é ¼ã§ãã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€å®‰æ˜“ã«å›ç­”ã‚’ç”Ÿæˆã›ãšã€ã€Œæã‚Œå…¥ã‚Šã¾ã™ãŒã€ã„ãŸã ã„ãŸæƒ…å ±ã‹ã‚‰ã§ã¯æ˜ç¢ºãªã”æ¡ˆå†…ãŒé›£ã—ã„çŠ¶æ³ã§ã™ã€‚ã€ã®ã‚ˆã†ã«ã€æ­£ç›´ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

        6.  **æƒ…å ±ã®é®®åº¦ã«é–¢ã™ã‚‹åˆ¤æ–­ã®ç¦æ­¢:**
            - ã‚ãªãŸã¯ã€æ ¹æ‹ æƒ…å ±ã®æ–°æ—§ã‚„æœ‰åŠ¹æ€§ã‚’**è‡ªå·±åˆ¤æ–­ã—ã¦ã¯ãªã‚Šã¾ã›ã‚“**ã€‚ã€Œã“ã®æƒ…å ±ã¯å¤ã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€ã¨ã„ã£ãŸæ¨æ¸¬ã¯ã€ãŸã¨ãˆã‚‚ã£ã¨ã‚‚ã‚‰ã—ãã¦ã‚‚å›ºãç¦ã˜ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
            - æƒ…å ±ãŒçŸ›ç›¾ã—ã¦ã„ã‚‹å ´åˆã¯ã€ãã®äº‹å®Ÿã®ã¿ã‚’å ±å‘Šã—ã€ã©ã¡ã‚‰ãŒæ­£ã—ã„ã‹ã‚’æ–­å®šã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚

        # ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
        {formatted_history}
        # ãŠå®¢æ§˜ã®ç¾åœ¨ã®è³ªå•
        {state['user_question']}
        # æ ¹æ‹ æƒ…å ±
        {state['retrieved_context']}

        # ä¸Šè¨˜ã®å³æ ¼ãªãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ç”Ÿæˆã—ãŸå›ç­” (å›ç­”ã®æœ¬æ–‡ã®ã¿ã‚’å‡ºåŠ›ã—ã€æ ¹æ‹ æƒ…å ±è‡ªä½“ã¯å«ã‚ãªã„ã“ã¨):
        """
        response = self._gen(prompt, temperature=0.0)
        return {"initial_answer": getattr(response, "text", "")}

    # Node 2.5: ä¼šè©±ã®ã¿ã®å›ç­”ã‚’ç”Ÿæˆã™ã‚‹
    def generate_conversational_answer(self, state: AgentState):
        self.logger.info("---ğŸ’¬ Node: generate_conversational_answer---")
        formatted_history = "".join(
            [f"ãŠå®¢æ§˜: {msg.content}\n" if isinstance(msg, HumanMessage) else f"AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {msg.content}\n"
             for msg in state['messages']]
        )

        prompt = f"""
        ã‚ãªãŸã¯ã€SmartHRã®è¦ªåˆ‡ãªã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ã€Œã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã€ã‚’å‚è€ƒã«ã€ãŠå®¢æ§˜ã®ç¾åœ¨ã®è³ªå•ã«å¯¾ã—ã¦è‡ªç„¶ãªä¼šè©±ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚

        # ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
        {formatted_history}
        # ãŠå®¢æ§˜ã®ç¾åœ¨ã®è³ªå•
        {state['user_question']}
        # å¿œç­”
        """
        response = self._gen(prompt)
        return {"final_answer": getattr(response, "text", "")}

    # â–¼â–¼â–¼ Node 2.8: è©•ä¾¡ãƒ»å†æ¤œç´¢è¨ˆç”» â–¼â–¼â–¼
    def grade_answer_and_plan(self, state: AgentState):
        self.logger.info("---ğŸ¤” Node: grade_answer_and_plan---")

        attempts = state.get('search_attempts', 0) + 1
        if attempts > 3:  # æœ€å¤§è©¦è¡Œå›æ•°
            self.logger.warning("   - æœ€å¤§æ¤œç´¢å›æ•°ã«é”ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã«é€²ã¿ã¾ã™ã€‚")
            return {"search_attempts": attempts, "sufficiency_decision": "sufficient"}

        prompt = f"""
        ã‚ãªãŸã¯ã€ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å›ç­”ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€ã«å¯¾ã—ã¦ã€ã€Œç”Ÿæˆã•ã‚ŒãŸå›ç­”ã€ãŒååˆ†ã«ç­”ãˆã‚‰ã‚Œã¦ã„ã‚‹ã‹ã€ãã‚Œã¨ã‚‚æƒ…å ±ä¸è¶³ã§ç­”ãˆã‚‰ã‚Œã¦ã„ãªã„ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚

        # åˆ¤æ–­åŸºæº–
        - å›ç­”ãŒã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã€Œæ˜ç¢ºãªã”æ¡ˆå†…ãŒé›£ã—ã„ã€ã¨ã„ã£ãŸè¶£æ—¨ã®å†…å®¹ã§ã‚ã‚‹å ´åˆã€æƒ…å ±ã¯**ä¸è¶³**ã—ã¦ã„ã¾ã™ã€‚
        - å›ç­”ãŒå…·ä½“çš„ãªè§£æ±ºç­–ã‚„æƒ…å ±ã‚’æä¾›ã—ã¦ã„ã‚‹å ´åˆã€æƒ…å ±ã¯**ååˆ†**ã§ã™ã€‚

        # åˆ¤æ–­å¾Œã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        - æƒ…å ±ãŒ**ååˆ†**ãªå ´åˆ: {{"status":"sufficient","next_query":null}}
        - æƒ…å ±ãŒ**ä¸è¶³**ã—ã¦ã„ã‚‹å ´åˆ: {{"status":"insufficient","next_query":"ä¸è¶³æƒ…å ±ã‚’å¾—ã‚‹ãŸã‚ã®æ–°ã—ã„å…·ä½“çš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒª"}}

        # å…¥åŠ›æƒ…å ±
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•: {state['user_question']}
        - ã“ã‚Œã¾ã§ã«æ¤œç´¢ã—ãŸæƒ…å ±: {state['retrieved_context']}
        - ç”Ÿæˆã•ã‚ŒãŸå›ç­”: {state['initial_answer']}

        # å‡ºåŠ› (JSONå½¢å¼ã®ã¿)
        """
        resp = self._gen(prompt, response_mime_type="application/json")
        try:
            result = json.loads(getattr(resp, "text", "") or "{}")
        except Exception:
            result = {}

        self.logger.info(f"   - è©•ä¾¡çµæœ: {result.get('status')}")
        if result.get('status') == 'insufficient':
            self.logger.info(f"   - æ¬¡ã®æ¤œç´¢ã‚¯ã‚¨ãƒª: {result.get('next_query')}")
            return {
                "sufficiency_decision": "insufficient",
                "current_query": result.get('next_query'),
                "search_attempts": attempts
            }
        else:
            return {
                "sufficiency_decision": "sufficient",
                "search_attempts": attempts
            }

    # Node 3: ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ (â˜… ç›£æŸ»ã¨æ¸…æ›¸ã‚’çµ±åˆ)
    def fact_check(self, state: AgentState):
        self.logger.info("---ğŸ”¬ Node: fact_check (and format)---")
        prompt = f"""
        ã‚ãªãŸã¯ã€ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®å›ç­”ã‚’ç›£æŸ»ã™ã‚‹ã€æ¥µã‚ã¦å³æ ¼ãªå“è³ªä¿è¨¼ï¼ˆQAï¼‰ã®å°‚é–€å®¶ã§ã™ã€‚
        ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã¯2ã¤ã‚ã‚Šã¾ã™ã€‚

        1.  **ç›£æŸ»:** ã€Œç”Ÿæˆã•ã‚ŒãŸå›ç­”ã€ãŒã€Œæ ¹æ‹ æƒ…å ±ã€ã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ã€ç‰¹ã«ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€ã®å‰æãŒèª¤ã£ã¦ã„ãªã„ã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
        2.  **æ¸…æ›¸ (ç›£æŸ»OKã®å ´åˆã®ã¿):** ã‚‚ã—ç›£æŸ»ã®çµæœãŒOK (is_grounded: true) ã ã£ãŸå ´åˆã€å›ç­”ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼æç¤ºç”¨ã®æœ€çµ‚å½¢å¼ï¼ˆæ ¹æ‹ ã®å¼•ç”¨ä»˜ãï¼‰ã«æ¸…æ›¸ã—ã¾ã™ã€‚

        # è©•ä¾¡å¯¾è±¡
        - **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•**: {state['user_question']}
        - **æ ¹æ‹ æƒ…å ±**: {state['retrieved_context']}
        - **ç”Ÿæˆã•ã‚ŒãŸå›ç­” (æœ¬æ–‡ã®ã¿)**: {state['initial_answer']}

        # ç›£æŸ»åŸºæº– (æœ€å„ªå…ˆ)
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã®å‰æï¼ˆä¾‹ï¼šã€ŒAã®å¾Œã«Bã‚’ã™ã‚‹ã€ï¼‰ãŒã€æ ¹æ‹ æƒ…å ±ï¼ˆä¾‹ï¼šã€ŒBã®å¾Œã«Aã‚’ã™ã‚‹ã€ï¼‰ã¨çŸ›ç›¾ã—ã¦ã„ã‚‹å ´åˆã€å›ç­”ãŒãã®çŸ›ç›¾ã‚’æŒ‡æ‘˜ã›ãšå‰æã‚’è‚¯å®šã—ã¦ã„ã‚Œã°ã€**NG**ã§ã™ã€‚
        - å›ç­”ã«ã€æ ¹æ‹ æƒ…å ±ã«ãªã„æƒ…å ±ã‚„æ‹¡å¤§è§£é‡ˆãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€**NG**ã§ã™ã€‚

        # å‡ºåŠ›å½¢å¼ (JSON)
        ç›£æŸ»ã®çµæœã€ä»¥ä¸‹ã®ã©ã¡ã‚‰ã‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

        ## 1. ç›£æŸ»ãŒOKã ã£ãŸå ´åˆ
        {{
            "is_grounded": true,
            "reason": "å›ç­”ã¯æ ¹æ‹ æƒ…å ±ã«åŸºã¥ã„ã¦ãŠã‚Šã€å‰æã®èª¤ã‚Šã‚‚ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
            "formatted_answer": "ï¼ˆã“ã“ã«ã€å›ç­”æœ¬æ–‡ã¨ã€Œ**æ ¹æ‹ æƒ…å ±:**ã€ã®å¼•ç”¨ãƒ–ãƒ­ãƒƒã‚¯ã‚’å«ã‚€ã€æ¸…æ›¸æ¸ˆã¿ã®æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ï¼‰"
        }}
        
        ## 2. ç›£æŸ»ãŒNGã ã£ãŸå ´åˆ
        {{
            "is_grounded": false,
            "reason": "ï¼ˆã“ã“ã«ã€NGã¨åˆ¤æ–­ã—ãŸå…·ä½“çš„ãªç†ç”±ã‚’è¨˜è¿°ã™ã‚‹ã€‚ä¾‹ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èª¤ã£ãŸå‰æã‚’è‚¯å®šã—ã¦ã„ã‚‹...ï¼‰",
            "formatted_answer": null
        }}

        # ã‚ãªãŸã®å‡ºåŠ› (JSONå½¢å¼ã®ã¿):
        """
        resp = self._gen(prompt, response_mime_type="application/json")
        try:
            data = json.loads(getattr(resp, "text", "") or "{}")
        except Exception:
            data = {"is_grounded": False, "reason": "JSON parse error", "formatted_answer": None}
            
        # ç›£æŸ»çµæœã‚’fact_check_resultã«ä¿å­˜
        # ã‚‚ã—ç›£æŸ»OKãªã‚‰ã€formatted_answer ã‚’ initial_answer ã«ä¸Šæ›¸ãã™ã‚‹
        if data.get("is_grounded") and data.get("formatted_answer"):
            self.logger.info("    - ç›£æŸ»OKã€‚æ¸…æ›¸ç‰ˆã®å›ç­”ã‚’æ ¼ç´ã—ã¾ã™ã€‚")
            return {
                "fact_check_result": data,
                "initial_answer": data["formatted_answer"] # â˜… æ¸…æ›¸ç‰ˆã§ä¸Šæ›¸ã
            }
        else:
            self.logger.info("    - ç›£æŸ»NGã€‚")
            return {"fact_check_result": data}



    # Node 6: rewrite_answer
    def rewrite_answer(self, state: AgentState):
        self.logger.info("---ğŸ”§ Node: rewrite_answer (fact check)---")
        reason = (state.get('fact_check_result') or {}).get('reason', '')
        prompt = f"""
        ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã§æŒ‡æ‘˜ã‚’å—ã‘ã¾ã—ãŸã€‚æŒ‡æ‘˜å†…å®¹ã‚’è¸ã¾ãˆã€å¿…ãšã€Œæ ¹æ‹ æƒ…å ±ã€ã®ã¿ã§å›ç­”ã‚’**ä¿®æ­£**ã—ã¦ãã ã•ã„ã€‚
        åŒæ™‚ã«ã€å›ç­”ã®æœ«å°¾ã«ã€ãã®å›ç­”ã®æ ¹æ‹ ã¨ãªã£ãŸã€Œæ ¹æ‹ æƒ…å ±ã€ã®ä¸»è¦ãªéƒ¨åˆ†ã‚’ã€Œ**æ ¹æ‹ æƒ…å ±:**ã€ã¨ã—ã¦ç®‡æ¡æ›¸ãã§å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {state['user_question']}
        # æ ¹æ‹ æƒ…å ±: {state['retrieved_context']}
        # åˆæœŸã®å›ç­”: {state['initial_answer']}
        # æŒ‡æ‘˜å†…å®¹: {reason}
        # ä¿®æ­£å¾Œã®å›ç­”:
        """
        resp = self._gen(prompt)
        return {"initial_answer": getattr(resp, "text", "")}
        
    # â–¼â–¼â–¼ æ–°è¦è¿½åŠ  (Node 7) â–¼â–¼â–¼
    # Node 7: add_citations (æ ¹æ‹ ä»˜ä¸ã®ã¿)
    def add_citations_to_answer(self, state: AgentState):
        self.logger.info("---âœ¨ Node: add_citations_to_answer (fact check OK)---")
        
        prompt = f"""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å›ç­”ãŒç”Ÿæˆã•ã‚Œã€ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‚’ãƒ‘ã‚¹ã—ã¾ã—ãŸã€‚
        ã“ã®å›ç­”ã‚’ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æç¤ºã™ã‚‹æœ€çµ‚å½¢å¼ã«æ¸…æ›¸ã—ã¦ãã ã•ã„ã€‚

        # æ¸…æ›¸ã®ãƒ«ãƒ¼ãƒ«
        1. ã€Œç”Ÿæˆã•ã‚ŒãŸå›ç­”ã€ã®è«–æ—¨ã¯ãã®ã¾ã¾ç¶­æŒã—ã¦ãã ã•ã„ã€‚
        2. å›ç­”ã®æœ«å°¾ã«ã€ãã®å›ç­”ã®æ ¹æ‹ ã¨ãªã£ãŸã€Œæ ¹æ‹ æƒ…å ±ã€ã®ä¸»è¦ãªéƒ¨åˆ†ã‚’ã€Œ**æ ¹æ‹ æƒ…å ±:**ã€ã¨ã—ã¦ç®‡æ¡æ›¸ãã§ã©ã®éƒ¨åˆ†ã‹ã‚ã‹ã‚‹ã‚ˆã†ã«å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚
        3. ã€Œæ ¹æ‹ æƒ…å ±ã€ã‹ã‚‰ã€å›ç­”ã®è£ä»˜ã‘ã¨**ç›´æ¥é–¢ä¿‚ã®ãªã„æƒ…å ±**ã¯å¼•ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {state['user_question']}
        # æ ¹æ‹ æƒ…å ±: {state['retrieved_context']}
        # ç”Ÿæˆã•ã‚ŒãŸå›ç­” (ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯æ¸ˆ): {state['initial_answer']}
        
        # æ¸…æ›¸å¾Œã®æœ€çµ‚å›ç­” (æ ¹æ‹ ã®å¼•ç”¨ã‚’å«ã‚€):
        """
        resp = self._gen(prompt)
        # æœ€çµ‚å½¢å¼ã®å›ç­”ã‚’ initial_answer ã«æ ¼ç´
        return {"initial_answer": getattr(resp, "text", "")}

    # Node 8: finalize_retrieval_response (â˜… ä¿®æ­£)
    def finalize_retrieval_response(self, state: AgentState):
        self.logger.info("---ğŸ Node: finalize_retrieval_response---")
        # ã“ã®æ™‚ç‚¹ã§ã® 'initial_answer' ã¯ã€Node 6 ã¾ãŸã¯ Node 7 ã«ã‚ˆã£ã¦
        # æ—¢ã«æ ¹æ‹ ãŒä»˜ä¸ã•ã‚ŒãŸã€Œæœ€çµ‚å›ç­”ã€ã«ãªã£ã¦ã„ã¾ã™ã€‚
        return {"final_answer": state['initial_answer']}

    # Node 9: finalize_conversational_response
    def finalize_conversational_response(self, state: AgentState):
        self.logger.info("---ğŸ Node: finalize_conversational_response---")
        return {"final_answer": state['final_answer']}


# ----------------------------------------------------------------
# 3. ã‚°ãƒ©ãƒ•æ§‹ç¯‰ (ä¿®æ­£æ¸ˆã¿)
# ----------------------------------------------------------------
def build_support_agent_graph(chatbot_instance: AdkChatbot):
    agent = SupportOperationAgent(chatbot_instance)
    workflow = StateGraph(AgentState)

    # ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
    workflow.add_node("route_query", agent.route_query)
    workflow.add_node("retrieve", agent.retrieve)
    workflow.add_node("generate_retrieval", agent.generate_initial_answer)
    workflow.add_node("grade_and_plan", agent.grade_answer_and_plan)
    workflow.add_node("generate_conversational", agent.generate_conversational_answer)
    workflow.add_node("fact_check", agent.fact_check)
    workflow.add_node("rewrite_fact", agent.rewrite_answer)

    # â–¼â–¼â–¼ æ–°è¦ãƒãƒ¼ãƒ‰ã‚’ã‚°ãƒ©ãƒ•ã«è¿½åŠ  â–¼â–¼â–¼
    #workflow.add_node("add_citations", agent.add_citations_to_answer)
    
    workflow.add_node("finalize_retrieval", agent.finalize_retrieval_response)
    workflow.add_node("finalize_conversational", agent.finalize_conversational_response)

    # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    workflow.set_entry_point("route_query")

    # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
    def decide_path(state: AgentState):
        return state["route_decision"]

    workflow.add_conditional_edges(
        "route_query",
        decide_path,
        {"retrieval": "retrieve", "conversational": "generate_conversational"}
    )

    # æƒ…å ±æ¤œç´¢ãƒ«ãƒ¼ãƒˆï¼ˆãƒ«ãƒ¼ãƒ—ï¼‰
    workflow.add_edge("retrieve", "generate_retrieval")
    workflow.add_edge("generate_retrieval", "grade_and_plan")

    # è©•ä¾¡ãƒãƒ¼ãƒ‰ã‹ã‚‰ã®åˆ†å²
    def should_research_again(state: AgentState):
        if state.get("sufficiency_decision") == "insufficient":
            return "retrieve"
        else:
            return "fact_check"

    workflow.add_conditional_edges("grade_and_plan", should_research_again)
    
    # ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‹ã‚‰ã®æ¡ä»¶åˆ†å² (â˜… ä¿®æ­£)
    def should_rewrite_or_finalize(state: AgentState): # é–¢æ•°åã‚’å¤‰æ›´
        if state['fact_check_result']['is_grounded']:
            # ãƒ‘ã‚¹ã—ãŸå ´åˆ -> 'finalize_retrieval' ã¸ç›´è¡Œ
            # (fact_check ãƒãƒ¼ãƒ‰ãŒæ¸…æ›¸å›ç­”ã‚’ initial_answer ã«æ ¼ç´æ¸ˆã¿)
            return "finalize_retrieval" 
        else:
            # å¤±æ•—ã—ãŸå ´åˆ -> æ—¢å­˜ã® 'rewrite_fact' ãƒãƒ¼ãƒ‰ã¸
            return "rewrite_fact"

    workflow.add_conditional_edges("fact_check", should_rewrite_or_finalize)

    # 'rewrite_fact' ã¯ 'finalize_retrieval' ã«åˆæµ
    workflow.add_edge("rewrite_fact", "finalize_retrieval")
    

    # æœ€çµ‚ãƒãƒ¼ãƒ‰
    workflow.add_edge("finalize_retrieval", END)
    workflow.add_edge("generate_conversational", "finalize_conversational")
    workflow.add_edge("finalize_conversational", END)

    return workflow.compile()


def build_graph(chatbot_instance: AdkChatbot):
    return build_support_agent_graph(chatbot_instance)