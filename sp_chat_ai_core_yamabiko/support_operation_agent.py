from .chat_engine_adk_bq import AdkChatbot
from google import genai
from google.genai import types


from typing import TypedDict, Annotated, List, Sequence
import operator
import json
from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ----------------------------------------------------------------
# 1. Stateã®å®šç¾© (ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‰Šé™¤ãƒ»æ•´ç†)
# ----------------------------------------------------------------
class AgentState(TypedDict):

    # â–¼â–¼â–¼ APPã‹ã‚‰ã®å—ã‘æ¸¡ã—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ â–¼â–¼â–¼
    conversation_id: str
    recognized_products: str
    
    # â–¼â–¼â–¼ ç®¡ç†ç”¨ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ â–¼â–¼â–¼
    session_id: str
    message_index: int
    user_question: str
    retrieved_context: str
    human_readable_context: str
    initial_answer: str
    fact_check_result: dict
    final_answer: str
    route_decision: str
    messages: Sequence[BaseMessage]

    # â–¼â–¼â–¼ æ¤œç´¢ãƒ«ãƒ¼ãƒ—ç®¡ç†ç”¨ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ â–¼â–¼â–¼
    current_query: str          # ç¾åœ¨ã®æ¤œç´¢ã‚¯ã‚¨ãƒª
    search_attempts: int        # æ¤œç´¢ã®è©¦è¡Œå›æ•°
    sufficiency_decision: str   # å›ç­”ãŒååˆ†ã‹ã©ã†ã‹ã®åˆ¤æ–­ ("sufficient" or "insufficient")

    # â–¼ å›³ã®ä¸‹å´ãƒ•ãƒ­ãƒ¼å¯¾å¿œç”¨
    conversation_phase: str      # "new" | "after_answer"
    followup_type: str           # "resolved" | "same_feature" | "different_feature" | "escalation"
    # â–¼â–¼â–¼ è¿½åŠ : ãƒŠãƒ¬ãƒƒã‚¸ä¸è¶³ãƒ•ãƒ©ã‚° â–¼â–¼â–¼
    is_knowledge_missing: bool      # Trueãªã‚‰ã€ŒãƒŠãƒ¬ãƒƒã‚¸0ä»¶ï¼†éå»ãƒ­ã‚°ã‚ã‚Šã€ã®çŠ¶æ…‹

    # â˜… è¿½åŠ : èãè¿”ã—ç™ºç”Ÿãƒ•ãƒ©ã‚°ï¼ˆãƒ­ã‚°è¨˜éŒ²ç”¨ï¼‰
    is_clarification_required: bool 
    # â˜… è¿½åŠ : é€šéã—ãŸãƒ«ãƒ¼ãƒˆã®å±¥æ­´è¨˜éŒ²ç”¨ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ»åˆ†æç”¨ï¼‰
    route_history: List[str]


# ----------------------------------------------------------------
# 2. Agentã‚¯ãƒ©ã‚¹ï¼ˆã‚·ãƒ³ãƒ—ãƒ«åŒ–ï¼‰
# ----------------------------------------------------------------
class SupportOperationAgent:
    def __init__(self, chatbot: AdkChatbot):
        self.chatbot = chatbot
        self.client = getattr(chatbot, "gclient", None) or genai.Client()
        self.model_name = "gemini-2.5-flash"
        self.logger = logging.getLogger(__name__)
        
        # â˜… ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãªã©ã‚’å‰Šé™¤

    # --- å…±é€šãƒ˜ãƒ«ãƒ‘ ---
    def _gen(self, prompt: str, *, response_mime_type: str | None = None, temperature: float | None = 0.0):
        cfg = types.GenerateContentConfig(
            response_mime_type=response_mime_type,
            temperature=0.0 if temperature is None else temperature,
        )
        return self.client.models.generate_content(
            model=self.model_name,
            contents=prompt,
            config=cfg,
        )

    def _append_resolution_check(self, answer: str) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®æœ€çµ‚å›ç­”ã®æœ«å°¾ã«ã€è§£æ±ºç¢ºèªã®ä¸€æ–‡ã‚’ä»˜ä¸ã™ã‚‹ã€‚"""
        tail = (
            "\n\n"
            "â€•â€•â€•â€•\n"
            "ä»Šå›ã®ã”æ¡ˆå†…ã§ã”ä¸æ˜ç‚¹ã¯è§£æ¶ˆã•ã‚Œã¾ã—ãŸã§ã—ã‚‡ã†ã‹ï¼Ÿ\n"
            "ãƒ»åŒã˜æ©Ÿèƒ½ã«ã¤ã„ã¦ã®è¿½åŠ ã®ã”è³ªå•ãŒã‚ã‚Œã°ã€ãã®ã¾ã¾ç¶šã‘ã¦ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚\n"
            "ãƒ»å°‚é–€ã®æ‹…å½“è€…ã¸ã®ãŠå–æ¬¡ãã‚’ã”å¸Œæœ›ã®å ´åˆã¯ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ã®é€£çµ¡ã‚’å¸Œæœ›ã™ã‚‹ãƒœã‚¿ãƒ³ã‚’ã”é¸æŠãã ã•ã„ã€‚"
            # ã€Œæ‹…å½“è€…ã¸ã®ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ã®æ–‡è¨€ã¯å‰Šé™¤
        )
        return (answer or "").rstrip() + tail

    # ----------------------------------------------------------
    # å…¥å£ãƒ«ãƒ¼ã‚¿ãƒ¼
    # ----------------------------------------------------------
    def entry_router(self, state: AgentState):
        self.logger.info("---ğŸšª Node: entry_router ---")
        messages = state.get("messages", [])
        has_ai_before = any(not isinstance(m, HumanMessage) for m in messages[:-1]) if messages else False
        
        if messages and isinstance(messages[-1], HumanMessage) and has_ai_before:
            phase = "after_answer"
        else:
            phase = "new"
        return {"conversation_phase": phase}
        
    # Node: æ„å›³åˆ†é¡
    def classify_intent(self, state: AgentState):
        self.logger.info("---ğŸ§­ Node: classify_intent---")
        user_question = state['messages'][-1].content
            
        prompt = f"""
        æ¬¡ã®ç™ºè©±ã®æ„å›³ã‚’1ã¤ã ã‘é¸ã³ã€æ—¥æœ¬èªã®ã¾ã¾JSONã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        - "retrieval": è£½å“ä»•æ§˜/æ“ä½œ/æ–™é‡‘ãªã©ã®æƒ…å ±ç…§ä¼šï¼ˆãƒŠãƒ¬ãƒƒã‚¸å‚ç…§ã§å›ç­”å¯èƒ½ï¼‰
        - "conversational": æŒ¨æ‹¶ãƒ»é›‘è«‡ãƒ»ãŠç¤¼ãªã©ï¼ˆæ¤œç´¢ä¸è¦ï¼‰
        ç™ºè©±: {user_question}
        å‡ºåŠ›: {{"intent":"retrieval"|"conversational"}}
        """
        resp = self._gen(prompt, response_mime_type="application/json")
        
        try:
            data = json.loads(getattr(resp, "text", "") or "{}")
            intent = data.get("intent", "retrieval")
        except Exception:
            intent = "retrieval"
        
        route_decision = "retrieval" if intent == "retrieval" else "conversational"
        
        return {
            "user_question": user_question,
            "route_decision": route_decision,
            "current_query": user_question,
            "search_attempts": 0,
            "retrieved_context": "",
        }

    # Node: ãƒãƒªã‚·ãƒ¼ã‚²ãƒ¼ãƒˆï¼ˆä¿®æ­£ï¼šã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ãªãã€Œå›ç­”ä¸å¯ã€ã¨ã—ã¦å‡¦ç†ï¼‰
    def policy_gate(self, state: AgentState):
        self.logger.info("---ğŸš§ Node: policy_gate (Simpified)---")
        target_q = state.get("current_query") or state["user_question"]

        prompt = f"""
        æ¬¡ã®è³ªå•ãŒãƒãƒªã‚·ãƒ¼ä¸Šã€AIãŒå›ç­”ã™ã¹ãã§ãªã„å†…å®¹ï¼ˆå€‹äººæƒ…å ±ç…§ä¼šã€å¥‘ç´„è©³ç´°ãªã©ï¼‰ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

        # åˆ¤å®šåŸºæº–
        ## ğŸš« å›ç­”ä¸å¯ (need_escalation: true)
        - ç‰¹å®šã®ä¼æ¥­ã®ã€Œå¥‘ç´„å†…å®¹ã€ã‚„ã€Œè«‹æ±‚é‡‘é¡ã€ã®ç¢ºèª
        - **ç‰¹å®šã®å€‹äººãƒ»å¾“æ¥­å“¡ã®ãƒ‡ãƒ¼ã‚¿ç…§ä¼šãƒ»æ“ä½œä¾é ¼** (ä¾‹:ã€ŒAã•ã‚“ã®çµ¦ä¸ã‚’æ•™ãˆã¦ã€ã€ŒBã•ã‚“ã‚’å‰Šé™¤ã—ã¦ã€)
        - æ³•å¾‹çš„ãªåˆ¤æ–­ãƒ»åŠ´å‹™ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°

        ## âœ… å›ç­”OK (need_escalation: false)
        - ä¸€èˆ¬çš„ãªæ©Ÿèƒ½ã®ä½¿ã„æ–¹ã€æ“ä½œæ‰‹é †ã€ä»•æ§˜ã®è³ªå•

        è³ªå•: {target_q}
        å‡ºåŠ›å½¢å¼: {{"need_escalation": true/false, "reason": "ç†ç”±"}}
        """
        
        r = self._gen(prompt, response_mime_type="application/json")
        data = json.loads(getattr(r, "text", "") or "{}")
        is_ng = data.get("need_escalation", False)
        reason = data.get("reason", "")

        if is_ng:
            self.logger.info(f"    - Policy Blocked: {reason}")
            # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã¯å»ƒæ­¢ã—ãŸãŸã‚ã€ã“ã“ã§ã€Œå›ç­”ä¸å¯ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œã£ã¦ä¼šè©±çµ‚äº†ãƒ«ãƒ¼ãƒˆã¸æµã™
            refusal_msg = (
                "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å€‹äººæƒ…å ±ã‚„å¥‘ç´„è©³ç´°ã«é–¢ã™ã‚‹ãŠå•ã„åˆã‚ã›ã€"
                "ã¾ãŸã¯æ³•çš„ãªåˆ¤æ–­ã‚’è¦ã™ã‚‹ã”è³ªå•ã«ã¯ã€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã¯ãŠç­”ãˆã§ãã¾ã›ã‚“ã€‚\n"
                "ãŠæ‰‹æ•°ã§ã™ãŒã€æ‹…å½“è€…ã¸ç›´æ¥ãŠå•ã„åˆã‚ã›ã„ãŸã ã‘ã¾ã™ã‚ˆã†ãŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚"
            )
            return {
                "route_decision": "conversational", # ä¼šè©±çµ‚äº†ãƒ«ãƒ¼ãƒˆã¸
                "final_answer": refusal_msg
            }

        # OKãªã‚‰å…ƒã®ãƒ«ãƒ¼ãƒˆã‚’ç¶­æŒ
        return {"route_decision": state.get("route_decision", "retrieval")}
        
    # â˜… escalate_to_human ãƒ¡ã‚½ãƒƒãƒ‰ã¯å®Œå…¨ã«å‰Šé™¤ã—ã¾ã—ãŸ â˜…

    # Node: æƒ…å ±æ¤œç´¢
    def retrieve(self, state: AgentState):
        self.logger.info("---ğŸ” Node: retrieve---")
        query = state['current_query']
        session_id = state.get('session_id')
        message_index = state.get('message_index')

        ai_context, human_context, search_meta = self.chatbot._get_information_for_query(
            query,
            session_id=session_id,
            message_index=message_index,
        )
        is_knowledge_missing = search_meta.get("is_knowledge_missing", False)
        
        existing_context = state.get('retrieved_context', '')
        updated_context = (existing_context + f"\n\n--- æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã®çµæœ ---\n" + (ai_context or "")).strip()

        existing_human_context = state.get('human_readable_context', '')
        updated_human_context = (existing_human_context + "\n" + (human_context or "")).strip() if existing_human_context else (human_context or "")
        history = state.get("route_history", []) + ["retrieve"]

        return {
            "retrieved_context": updated_context,
            "human_readable_context": updated_human_context,
            "is_knowledge_missing": is_knowledge_missing,
            "route_history": history,
        }

    # Node: æ›–æ˜§æ€§ãƒã‚§ãƒƒã‚¯ (â˜…ã“ã“ã‚’ä¿®æ­£)
    def check_ambiguity(self, state: AgentState):
        self.logger.info("---âš–ï¸ Node: check_ambiguity---")
        history = state.get("route_history", []) + ["check_ambiguity"]

        attempts = state.get("search_attempts", 0)
        max_attempts = 3
        is_last_try = (attempts >= max_attempts)

        # ç›´å‰ã®AIå›ç­”ï¼ˆGradeã§Insufficientã¨ã•ã‚ŒãŸã‚‚ã®ï¼‰
        initial_answer = state.get("initial_answer", "")

        # â˜… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿®æ­£:
        # å˜ãªã‚‹æƒ…å ±ä¸è¶³(Clear)ãªã®ã‹ã€æ–‡è„ˆçš„ã«æ›–æ˜§ã§èãè¿”ã™å¿…è¦ãŒã‚ã‚‹(Ambiguous)ã®ã‹ã‚’å³å¯†ã«åˆ¤å®š
        # éå»ãƒ­ã‚°ç”±æ¥ã®ã€Œä¸é©åˆ‡ãªèãè¿”ã—ã€ã‚’é™¤å¤–ã™ã‚‹ãŸã‚ã€æ¤œç´¢çµæœ(retrieved_context)ã«ç…§ã‚‰ã—ã¦å¦¥å½“æ€§ã‚‚ãƒã‚§ãƒƒã‚¯
        prompt = f"""
        ç¾åœ¨ã®å›ç­”å€™è£œã¯ã€Œä¸ååˆ†ã€ã¨åˆ¤å®šã•ã‚Œã¾ã—ãŸã€‚
        ã“ã‚ŒãŒã€Œæ¤œç´¢ä¸è¶³ã€ã«ã‚ˆã‚‹ã‚‚ã®ã‹ã€è³ªå•ãŒã€Œæ›–æ˜§ã€ã§çµã‚Šè¾¼ã‚ãªã„ãŸã‚ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

        # åˆ¤æ–­ææ–™
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {state['user_question']}
        - ç¾åœ¨ã®æ¤œç´¢çµæœ: {state['retrieved_context'][:5000]}
        - ç”Ÿæˆã•ã‚ŒãŸå›ç­”å€™è£œ: {initial_answer}

        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ±ºå®š
        1. **Ambiguous (èãè¿”ã—ãŒå¿…è¦)**: 
           - æ¤œç´¢çµæœã«ã€ŒAã®å ´åˆã¯Xã€Bã®å ´åˆã¯Yã€ã¨ã„ã£ãŸåˆ†å²æƒ…å ±ãŒã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç¾çŠ¶ãŒä¸æ˜ãªãŸã‚å›ç­”ã‚’ä¸€æ„ã«çµã‚Œãªã„å ´åˆã€‚
           - **æ³¨æ„**: ç”Ÿæˆã•ã‚ŒãŸå›ç­”å€™è£œãŒã€Œèãè¿”ã—ã€ã‚’è¡Œã£ã¦ã„ã¦ã‚‚ã€ãã‚ŒãŒæ¤œç´¢çµæœã«åŸºã¥ã‹ãªã„ä¸é©åˆ‡ãªã‚‚ã®ï¼ˆä»–è£½å“ã®ä»•æ§˜ãªã©ï¼‰ã§ã‚ã‚‹å ´åˆã¯ã€ã“ã“ã‚’é¸ã°ãšã« "Clear" (å†æ¤œç´¢) ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚
           
        2. **Clear (å†æ¤œç´¢ãŒå¿…è¦)**: 
           - å˜ã«æƒ…å ±ãŒè¦‹ã¤ã‹ã£ã¦ã„ãªã„å ´åˆã€‚
           - ç”Ÿæˆã•ã‚ŒãŸå›ç­”å€™è£œã®ã€Œèãè¿”ã—ã€ãŒçš„å¤–ã‚Œãªå ´åˆã€‚

        # å‡ºåŠ› (JSON)
        {{
            "status": "ambiguous" | "clear",
            "clarification_question": "ambiguousã®å ´åˆã®ã¿ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ä¸å¯§ãªèãè¿”ã—æ–‡ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        }}
        """
        
        resp = self._gen(prompt, response_mime_type="application/json")
        data = json.loads(getattr(resp, "text", "") or "{}")
        status = data.get("status", "clear")

        if status == "ambiguous":
             self.logger.info("    - åˆ¤å®š: Ambiguous -> ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸èãè¿”ã—ã‚’å®Ÿè¡Œ")
             clarification_msg = data.get("clarification_question", "è©³ç´°ã‚’ãŠèã‹ã›ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ")
             
             return {
                 "route_decision": "ambiguous",
                 "final_answer": clarification_msg,
                 "is_clarification_required": True, # â˜… ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
                 "route_history": history
             }
        
        # Ambiguousã§ãªã„å ´åˆ
        if is_last_try:
             # ã‚‚ã†æ¤œç´¢å›æ•°ä¸Šé™ãªã‚‰è«¦ã‚ã‚‹
             self.logger.info("    - åˆ¤å®š: Clearã ãŒå›æ•°åˆ‡ã‚Œ -> çµ‚äº†")
             fallback_msg = (
                 "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ä½•åº¦ã‹æ¤œç´¢ã‚’è©¦ã¿ã¾ã—ãŸãŒã€"
                 "ã”è³ªå•ã«å¯¾ã™ã‚‹æ˜ç¢ºãªæƒ…å ±ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
             )
             return {
                 "route_decision": "ambiguous", # å¼·åˆ¶çµ‚äº†ãƒ«ãƒ¼ãƒˆã¸
                 "final_answer": fallback_msg,
                 "is_clarification_required": False,
                 "route_history": history
             }
        
        # ã¾ã æ¤œç´¢ã§ãã‚‹ãªã‚‰å†æ¤œç´¢
        self.logger.info("    - åˆ¤å®š: Clear -> å†æ¤œç´¢ãƒ«ãƒ¼ãƒ—")
        return {
            "route_decision": "clear",
            "is_clarification_required": False,
            "route_history": history
        }

    # Node: å›ç­”ç”Ÿæˆ
    # Node 2: å›ç­”ç”Ÿæˆ
    def generate_initial_answer(self, state: AgentState):
        self.logger.info("---âœï¸ Node: generate_initial_answer (with retrieval)---")
        formatted_history = "".join(
            [f"ãŠå®¢æ§˜: {msg.content}\n" if isinstance(msg, HumanMessage) else f"AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {msg.content}\n"
             for msg in state['messages']]
        )

        prompt = f"""
        ã‚ãªãŸã¯ã€SmartHRã®ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒãƒ¼ãƒ ã«æ‰€å±ã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
        æä¾›ã•ã‚ŒãŸã€Œæ ¹æ‹ æƒ…å ±ã€ã«åŸºã¥ã„ã¦ã€æ­£ç¢ºã‹ã¤è¦ªåˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

        # å›ç­”ç”Ÿæˆã®ãƒ«ãƒ¼ãƒ«
        1. **æƒ…å ±ã®çµ±åˆ:** - ã€Œé–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ã€ã‚„ã€Œé¡ä¼¼éå»å›ç­”ã€ã‹ã‚‰ã€è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’æ¢ã—ã¦ãã ã•ã„ã€‚
           - å®Œå…¨ãªä¸€è‡´ï¼ˆã€Œã¯ã„ã€å¯èƒ½ã§ã™ã€ãªã©ï¼‰ãŒãªãã¦ã‚‚ã€**æ©Ÿèƒ½ã®ä»•æ§˜ã‚„æ“ä½œæ‰‹é †ã®è¨˜è¿°ã‹ã‚‰ã€è³ªå•ã«å¯¾ã™ã‚‹ç­”ãˆãŒè«–ç†çš„ã«å°ãå‡ºã›ã‚‹å ´åˆ**ã¯ã€ãã‚Œã‚’å›ç­”ã¨ã—ã¦æç¤ºã—ã¦ãã ã•ã„ã€‚
           - ä¾‹: è³ªå•ã€Œç®¡ç†è€…ã¯ç·¨é›†ã§ãã‚‹ã‹ï¼Ÿã€ã«å¯¾ã—ã€ãƒŠãƒ¬ãƒƒã‚¸ã«ã€Œç·¨é›†ç”»é¢ã‹ã‚‰æ›´æ–°ã§ãã¾ã™ã€ã¨ã‚ã‚Œã°ã€ã€Œã¯ã„ã€ç·¨é›†ç”»é¢ã‹ã‚‰æ›´æ–°å¯èƒ½ã§ã™ã€ã¨å›ç­”ã—ã¦æ§‹ã„ã¾ã›ã‚“ã€‚

        2. **æƒ…å ±æºã®å„ªå…ˆ:**
           - ã€Œé–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ã€ã®æƒ…å ±ã‚’æœ€å„ªå…ˆã—ã¦ãã ã•ã„ã€‚ã€Œé¡ä¼¼éå»å›ç­”ã€ã¯è£œè¶³ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚

        3. **æ¨æ¸¬ã®ç¯„å›²:**
           - æ ¹æ‹ æƒ…å ±ã«å…¨ãè¨˜è¿°ãŒãªã„æ©Ÿèƒ½ã‚„ä»•æ§˜ã«ã¤ã„ã¦ã¯ã€æ±ºã—ã¦å‰µä½œã—ãªã„ã§ãã ã•ã„ã€‚
           - ãŸã ã—ã€ä¸€èˆ¬çš„ãªæ“ä½œï¼ˆã€Œä¿å­˜ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã€ãªã©ï¼‰ã‚„ã€æ–‡è„ˆä¸Šæ˜ã‚‰ã‹ãªä¸»èªï¼ˆã€Œæ“ä½œç”»é¢ã€ã¨ã„ãˆã°é€šå¸¸ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼/ç®¡ç†è€…ãŒæ“ä½œã™ã‚‹ï¼‰ã«ã¤ã„ã¦ã¯ã€è£œã£ã¦èª¬æ˜ã—ã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚

        4. **æƒ…å ±ä¸è¶³ã®å ´åˆ:**
           - ä¸Šè¨˜ã‚’è¸ã¾ãˆã¦ã‚‚ç­”ãˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã¿ã€ã€Œæã‚Œå…¥ã‚Šã¾ã™ãŒã€ã„ãŸã ã„ãŸæƒ…å ±ã‹ã‚‰ã§ã¯æ˜ç¢ºãªã”æ¡ˆå†…ãŒé›£ã—ã„çŠ¶æ³ã§ã™ã€‚ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚

        # ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
        {formatted_history}
        # ãŠå®¢æ§˜ã®ç¾åœ¨ã®è³ªå•
        {state['user_question']}
        # æ ¹æ‹ æƒ…å ±
        {state['retrieved_context']}

        # å›ç­”:
        """
        response = self._gen(prompt, temperature=0.0)
        return {"initial_answer": getattr(response, "text", "")}

    # Node: ä¼šè©±ã®ã¿ã®å›ç­”
    def generate_conversational_answer(self, state: AgentState):
        # ã‚‚ã—policy_gateã§æ—¢ã«final_answerãŒè¨­å®šã•ã‚Œã¦ã„ãŸã‚‰ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—
        if state.get("final_answer"):
            return {}

        self.logger.info("---ğŸ’¬ Node: generate_conversational_answer---")
        
        formatted_history = "".join(
            [f"ãŠå®¢æ§˜: {msg.content}\n" if isinstance(msg, HumanMessage) else f"AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {msg.content}\n"
             for msg in state['messages']]
        )

        prompt = f"""
        ã‚ãªãŸã¯ã€SmartHRã®è¦ªåˆ‡ãªã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ã€Œã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã€ã‚’å‚è€ƒã«ã€ãŠå®¢æ§˜ã®ç¾åœ¨ã®è³ªå•ã«å¯¾ã—ã¦è‡ªç„¶ãªä¼šè©±ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚

        # æ³¨æ„äº‹é …
        - ã‚‚ã—ãŠå®¢æ§˜ãŒã€Œæ‹…å½“è€…ã«ã¤ãªã„ã§ã»ã—ã„ã€ã€Œé›»è©±ã—ãŸã„ã€ç­‰ã®è¦æœ›ã‚’å‡ºã—ã¦ã„ã‚‹å ´åˆã¯ã€
          ã€Œç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç¾åœ¨ã¯AIã«ã‚ˆã‚‹è‡ªå‹•å¿œç­”ã®ã¿ã¨ãªã£ã¦ãŠã‚Šã¾ã™ã€‚ã“ã®ãƒãƒ£ãƒƒãƒˆã§è§£æ±ºã§ãã‚‹ã“ã¨ãŒã‚ã‚Œã°ãŠæ•™ãˆãã ã•ã„ã€
          ã¨ã„ã£ãŸè¶£æ—¨ã§ã€ä¸å¯§ã«ãŠæ–­ã‚Šã—ã¦ãã ã•ã„ã€‚

        # ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
        {formatted_history}
        # ãŠå®¢æ§˜ã®ç¾åœ¨ã®è³ªå•
        {state['user_question']}
        # å¿œç­”
        """
        response = self._gen(prompt)
        return {"final_answer": getattr(response, "text", "")}


    # Node: è©•ä¾¡ãƒ»è¨ˆç”»
    def grade_answer_and_plan(self, state: AgentState):
        self.logger.info("---ğŸ¤” Node: grade_answer_and_plan---")
        attempts = state.get("search_attempts", 0) + 1
        
        # ç°¡æ˜“å®Ÿè£…: å›ç­”ãŒç©ºãªã‚‰insufficient
        if not state.get("initial_answer"):
             return {"sufficiency_decision": "insufficient", "search_attempts": attempts}

        prompt = f"""
        ã‚ãªãŸã¯ã€ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å›ç­”ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹å“è³ªç®¡ç†è€…ã§ã™ã€‚
        ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€ã«å¯¾ã—ã¦ã€ã€Œç”Ÿæˆã•ã‚ŒãŸå›ç­”ã€ãŒè§£æ±ºç­–ã‚’æç¤ºã§ãã¦ã„ã‚‹ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

        # å…¥åŠ›æƒ…å ±
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {state['user_question']}
        - ç”Ÿæˆã•ã‚ŒãŸå›ç­”: {state['initial_answer']}

        # åˆ¤å®šåŸºæº–
        1. **Sufficient (ååˆ†)**:
           - è³ªå•ã«å¯¾ã™ã‚‹å…·ä½“çš„ãªæ‰‹é †ã€è§£æ±ºç­–ã€ã¾ãŸã¯Yes/NoãŒæç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚
           - **ã€é‡è¦ä¾‹å¤–ã€‘**: å…¬å¼ãƒãƒ‹ãƒ¥ã‚¢ãƒ«(ãƒŠãƒ¬ãƒƒã‚¸)ãŒãªãã¦ã‚‚ã€éå»ã®å•ã„åˆã‚ã›å±¥æ­´(Past QA)ã‚’å¼•ç”¨ã—ã¦ã€å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼è§£æ±ºç­–ã‚„å›é¿ç­–ã‚’æç¤ºã§ãã¦ã„ã‚‹å ´åˆã¯ã€ã€Œååˆ†ã€ã¨åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
           - å›ç­”å†…ã§ã€Œã€œã§ã—ã‚‡ã†ã‹ï¼Ÿã€ã¨çŠ¶æ³ç¢ºèªã®è³ªå•ã‚’ã—ã¦ã„ã‚‹å ´åˆã‚‚ã€ä¼šè©±ã‚’é€²ã‚ã‚‹ãŸã‚ã«ã€Œååˆ†ã€ã¨åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

        2. **Insufficient (ä¸è¶³)**:
           - ã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ã„ã†çµè«–ã®å ´åˆã€‚
           - **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦çŠ¶æ³ç¢ºèªã‚„æƒ…å ±ã®è¿½åŠ ã‚’æ±‚ã‚ã¦ã„ã‚‹å ´åˆï¼ˆèãè¿”ã—ï¼‰ã€‚**
           - è³ªå•ã¨å›ç­”ãŒã‹ã¿åˆã£ã¦ã„ãªã„å ´åˆã€‚

        # å‡ºåŠ› (JSONå½¢å¼ã®ã¿)
        {{
            "status": "sufficient" | "insufficient",
            "next_query": "insufficientã®å ´åˆã®ã¿ã€æ¬¡ã«æ¤œç´¢ã™ã¹ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆâ€»ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®è³ªå•æ–‡ã§ã¯ãªãã€æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã«å…¥åŠ›ã™ã‚‹å˜èªï¼‰"
        }}
        """
        resp = self._gen(prompt, response_mime_type="application/json")
        result = json.loads(getattr(resp, "text", "") or "{}")
        status = result.get("status", "sufficient")
        next_q = result.get("next_query", state["current_query"])

        if status == "insufficient":
            return {
                "sufficiency_decision": "insufficient",
                "current_query": next_q,
                "search_attempts": attempts
            }
        
        return {"sufficiency_decision": "sufficient", "search_attempts": attempts}

    # Node: ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯
    def fact_check(self, state: AgentState):
        self.logger.info("---ğŸ”¬ Node: fact_check---")
        # ï¼ˆå…ƒã®å®Ÿè£…ã¨åŒã˜ï¼‰
        # ç°¡ç•¥åŒ–ã®ãŸã‚ã€å¸¸ã«OKã¨ã—ã¦é€šã™ã‹ã€å…ƒã®å³å¯†ãªãƒã‚§ãƒƒã‚¯ã‚’æ®‹ã™ã‹ã¯è‡ªç”±ã§ã™ãŒã€
        # ã“ã“ã§ã¯å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒã—ã¾ã™ã€‚
        prompt = f"""
        ã‚ãªãŸã¯ã€ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®å›ç­”ã‚’ç›£æŸ»ã™ã‚‹ã€æ¥µã‚ã¦å³æ ¼ãªå“è³ªä¿è¨¼ï¼ˆQAï¼‰ã®å°‚é–€å®¶ã§ã™ã€‚
        ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã¯2ã¤ã‚ã‚Šã¾ã™ã€‚

        1.  **ç›£æŸ»:** ã€Œç”Ÿæˆã•ã‚ŒãŸå›ç­”ã€ãŒã€Œæ ¹æ‹ æƒ…å ±ã€ã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ã€ç‰¹ã«ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€ã®å‰æãŒèª¤ã£ã¦ã„ãªã„ã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
        2.  **æ¸…æ›¸ (ç›£æŸ»OKã®å ´åˆã®ã¿):** ã‚‚ã—ç›£æŸ»ã®çµæœãŒOK (is_grounded: true) ã ã£ãŸå ´åˆã€å›ç­”ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼æç¤ºç”¨ã®æœ€çµ‚å½¢å¼ï¼ˆæ ¹æ‹ ã®å¼•ç”¨ä»˜ãï¼‰ã«æ¸…æ›¸ã—ã¾ã™ã€‚ãã®éš›ã€å¿…ãšæ ¹æ‹ æƒ…å ±ã«å«ã¾ã‚Œã‚‹URLã‚’ä½¿ç”¨ã—ã€å›ç­”æœ¬æ–‡ä¸­ã®é©åˆ‡ãªå˜èªã«ãƒã‚¤ãƒ‘ãƒ¼ãƒªãƒ³ã‚¯ã‚’é©ç”¨ã—ã¦ãã ã•ã„ã€‚å½¢å¼ã¯å¿…ãš **Markdown** `[è¡¨ç¤ºã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆ](URL)` ã¨ã—ã¦ãã ã•ã„ã€‚ï¼ˆä¾‹: ã€Œæ“ä½œæ‰‹é †ã«ã¤ã„ã¦ã¯[ç®¡ç†è€…ãƒãƒ‹ãƒ¥ã‚¢ãƒ«](https://...)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€ï¼‰

        # è©•ä¾¡å¯¾è±¡
        - **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•**: {state['user_question']}
        - **æ ¹æ‹ æƒ…å ±**: {state['retrieved_context']}
        - **ç”Ÿæˆã•ã‚ŒãŸå›ç­” (æœ¬æ–‡ã®ã¿)**: {state['initial_answer']}

        # ç›£æŸ»åŸºæº– (æœ€å„ªå…ˆ)
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã®å‰æï¼ˆä¾‹ï¼šã€ŒAã®å¾Œã«Bã‚’ã™ã‚‹ã€ï¼‰ãŒã€æ ¹æ‹ æƒ…å ±ï¼ˆä¾‹ï¼šã€ŒBã®å¾Œã«Aã‚’ã™ã‚‹ã€ï¼‰ã¨çŸ›ç›¾ã—ã¦ã„ã‚‹å ´åˆã€å›ç­”ãŒãã®çŸ›ç›¾ã‚’æŒ‡æ‘˜ã›ãšå‰æã‚’è‚¯å®šã—ã¦ã„ã‚Œã°ã€**NG**ã§ã™ã€‚
        - å›ç­”ã«ã€æ ¹æ‹ æƒ…å ±ã«ãªã„æƒ…å ±ã‚„æ‹¡å¤§è§£é‡ˆãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€**NG**ã§ã™ã€‚

        # å‡ºåŠ›å½¢å¼ (JSON)
        ç›£æŸ»ã®çµæœã€ä»¥ä¸‹ã®ã©ã¡ã‚‰ã‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

        ## 1. ç›£æŸ»ãŒOKã ã£ãŸå ´åˆ
        {{
            "is_grounded": true,
            "reason": "å›ç­”ã¯æ ¹æ‹ æƒ…å ±ã«åŸºã¥ã„ã¦ãŠã‚Šã€å‰æã®èª¤ã‚Šã‚‚ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
            "formatted_answer": "ï¼ˆã“ã“ã«ã€å›ç­”æœ¬æ–‡ã¨ã€Œ**æ ¹æ‹ æƒ…å ±:**ã€ã®å¼•ç”¨ãƒ–ãƒ­ãƒƒã‚¯ã‚’å«ã‚€ã€æ¸…æ›¸æ¸ˆã¿ã®æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ï¼‰"
        }}
        
        ## 2. ç›£æŸ»ãŒNGã ã£ãŸå ´åˆ
        {{
            "is_grounded": false,
            "reason": "ï¼ˆã“ã“ã«ã€NGã¨åˆ¤æ–­ã—ãŸå…·ä½“çš„ãªç†ç”±ã‚’è¨˜è¿°ã™ã‚‹ã€‚ä¾‹ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èª¤ã£ãŸå‰æã‚’è‚¯å®šã—ã¦ã„ã‚‹...ï¼‰",
            "formatted_answer": null
        }}

        # ã‚ãªãŸã®å‡ºåŠ› (JSONå½¢å¼ã®ã¿):
        """
        resp = self._gen(prompt, response_mime_type="application/json")
        data = json.loads(getattr(resp, "text", "") or "{}")
        
        if data.get("is_grounded"):
            return {"fact_check_result": data, "initial_answer": data.get("formatted_answer", "")}
        else:
            return {"fact_check_result": data}

    # Node 6: rewrite_answer (ä¿®æ­£ç‰ˆ)
    def rewrite_answer(self, state: AgentState):
        self.logger.info("---ğŸ”§ Node: rewrite_answer (fact check)---")
        reason = (state.get('fact_check_result') or {}).get('reason', '')
        
        # â˜… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿®æ­£: ã“ã¡ã‚‰ã‚‚Markdownå½¢å¼ã‚’å¼·åˆ¶
        prompt = f"""
        ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã§æŒ‡æ‘˜ã‚’å—ã‘ã¾ã—ãŸã€‚æŒ‡æ‘˜å†…å®¹ã‚’è¸ã¾ãˆã€å¿…ãšã€Œæ ¹æ‹ æƒ…å ±ã€ã®ã¿ã§å›ç­”ã‚’**ä¿®æ­£**ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æç¤ºã™ã‚‹æœ€çµ‚å½¢å¼ã«æ¸…æ›¸ã—ã¦ãã ã•ã„ã€‚

        # ãƒªãƒ³ã‚¯åŸ‹ã‚è¾¼ã¿ã®ãƒ«ãƒ¼ãƒ« (æœ€é‡è¦)
        æ ¹æ‹ æƒ…å ±ã«å«ã¾ã‚Œã‚‹URLã‚’ä½¿ç”¨ã—ã€å›ç­”æœ¬æ–‡ä¸­ã®é©åˆ‡ãªå˜èªã«ãƒã‚¤ãƒ‘ãƒ¼ãƒªãƒ³ã‚¯ã‚’é©ç”¨ã—ã¦ãã ã•ã„ã€‚
        å½¢å¼ã¯å¿…ãš **Markdown** `[è¡¨ç¤ºã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆ](URL)` ã¨ã—ã¦ãã ã•ã„ã€‚
        ï¼ˆä¾‹: ã€Œæ“ä½œæ‰‹é †ã«ã¤ã„ã¦ã¯[ç®¡ç†è€…ãƒãƒ‹ãƒ¥ã‚¢ãƒ«](https://...)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€ï¼‰

        # æ§‹æˆæŒ‡ç¤º
        1. ä¿®æ­£å¾Œã®å›ç­”æœ¬æ–‡ï¼ˆãƒªãƒ³ã‚¯åŸ‹ã‚è¾¼ã¿æ¸ˆã¿ï¼‰
        2. å›ç­”ã®æœ«å°¾ã«ã€æ ¹æ‹ ã¨ãªã£ãŸæƒ…å ±ã®è¦ç´„ã‚’ã€Œ**æ ¹æ‹ æƒ…å ±:**ã€ã¨ã—ã¦ç®‡æ¡æ›¸ãã§è¨˜è¼‰ã€‚

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {state['user_question']}
        # æ ¹æ‹ æƒ…å ±: {state['retrieved_context']}
        # åˆæœŸã®å›ç­”: {state['initial_answer']}
        # æŒ‡æ‘˜å†…å®¹: {reason}
        
        # ä¿®æ­£å¾Œã®å›ç­”:
        """
        resp = self._gen(prompt)
        return {"initial_answer": getattr(resp, "text", "")}

    # Node: æœ€çµ‚åŒ– (Retrieval)
    def finalize_retrieval_response(self, state: AgentState):
        self.logger.info("---ğŸ Node: finalize_retrieval_response---")
        base = state["initial_answer"]
        final = self._append_resolution_check(base)
        return {"final_answer": final}

    # Node: æœ€çµ‚åŒ– (Conversational)
    def finalize_conversational_response(self, state: AgentState):
        self.logger.info("---ğŸ Node: finalize_conversational_response---")
        return {"final_answer": state['final_answer']}

    # Node: ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—åˆ†é¡
    def followup_classifier(self, state: AgentState):
        self.logger.info("---ğŸ” Node: followup_classifier ---")
        last_user = state["messages"][-1].content
        
        # ç›´å‰ã®ã‚¯ã‚¨ãƒªã‚’å–å¾—ï¼ˆæ–‡è„ˆç¶­æŒã®ãŸã‚ï¼‰
        previous_query = state.get("current_query", "")

        prompt = f"""
        æ¬¡ã®ãŠå®¢æ§˜ã®ç™ºè©±ã‚¿ã‚¤ãƒ—ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚
        - "resolved": è§£æ±ºå ±å‘Šãƒ»ãŠç¤¼
        - "followup": è¿½åŠ è³ªå•ï¼ˆåŒã˜è©±é¡Œã®æ·±æ˜ã‚Šï¼‰
        - "new_topic": å…¨ãåˆ¥ã®è©±é¡Œã¸ã®è»¢æ›
        - "escalation": æ‹…å½“è€…ã¸ç¹‹ã„ã§ã»ã—ã„ã¨ã„ã†è¦æœ›
        ç™ºè©±: {last_user}
        å‡ºåŠ›: {{"decision": "resolved"|"followup"|"new_topic"|"escalation"}}
        """
        r = self._gen(prompt, response_mime_type="application/json")
        data = json.loads(getattr(r, "text", "") or "{}")
        decision = data.get("decision", "followup")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ãã®ã¾ã¾ä½¿ã†
        next_query = last_user

        if decision == "resolved":
            route = "conversational"
        elif decision == "escalation":
            # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¸Œæœ›æ™‚ã¯ä¼šè©±ãƒ«ãƒ¼ãƒˆã¸ï¼ˆAIãŒæ–­ã‚Šã‚’å…¥ã‚Œã‚‹ï¼‰
            route = "conversational"
        elif decision == "followup":
            # â˜…é‡è¦ä¿®æ­£: åŒã˜è©±é¡Œã®æ·±æ˜ã‚Šãªã‚‰ã€å‰å›ã®ã‚¯ã‚¨ãƒªã¨çµåˆã—ã¦æ¤œç´¢ç²¾åº¦ã‚’ä¿ã¤
            # ä¾‹: "å¾“æ¥­å“¡ç™»éŒ²" + "ä¸€æ‹¬ã§ã§ãã‚‹ï¼Ÿ"
            route = "retrieval"
            if previous_query:
                next_query = f"{previous_query} {last_user}"
        else: # new_topic
            # è©±é¡Œè»¢æ›ãªã‚‰ã€æ–°ã—ã„ç™ºè¨€ã ã‘ã§æ¤œç´¢ã™ã‚‹
            route = "retrieval"
            next_query = last_user
            
        return {
            "route_decision": route,
            "user_question": last_user,
            "current_query": next_query # æ–‡è„ˆè€ƒæ…®æ¸ˆã¿ã®ã‚¯ã‚¨ãƒª
        }


# ----------------------------------------------------------------
# 3. ã‚°ãƒ©ãƒ•æ§‹ç¯‰ (escalate_to_human ãƒãƒ¼ãƒ‰å‰Šé™¤ç‰ˆ)
# ----------------------------------------------------------------
def build_support_agent_graph(chatbot_instance: AdkChatbot):
    agent = SupportOperationAgent(chatbot_instance)
    workflow = StateGraph(AgentState)

    # === ãƒãƒ¼ãƒ‰ç™»éŒ² ===
    workflow.add_node("entry_router", agent.entry_router)
    workflow.add_node("classify_intent", agent.classify_intent)
    workflow.add_node("followup_classifier", agent.followup_classifier)
    workflow.add_node("policy_gate", agent.policy_gate)
    # workflow.add_node("escalate_to_human", agent.escalate_to_human)  <-- å‰Šé™¤
    
    workflow.add_node("check_ambiguity", agent.check_ambiguity) 
    workflow.add_node("generate_conversational", agent.generate_conversational_answer)
    workflow.add_node("finalize_conversational", agent.finalize_conversational_response)
    workflow.add_node("retrieve", agent.retrieve)
    workflow.add_node("generate_retrieval", agent.generate_initial_answer)
    workflow.add_node("grade_and_plan", agent.grade_answer_and_plan)
    workflow.add_node("fact_check", agent.fact_check)
    workflow.add_node("rewrite_fact", agent.rewrite_answer)
    workflow.add_node("finalize_retrieval", agent.finalize_retrieval_response)

    # === ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ ===
    workflow.set_entry_point("entry_router")

    # 1. å…¥å£
    def entry_route(state: AgentState):
        return "after_answer" if state.get("conversation_phase") == "after_answer" else "new"

    workflow.add_conditional_edges("entry_router", entry_route, {
        "new": "classify_intent",
        "after_answer": "followup_classifier",
    })

    # 2. æ„å›³åˆ†é¡ -> ãƒãƒªã‚·ãƒ¼
    workflow.add_edge("classify_intent", "policy_gate")

    # 3. ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ— -> ãƒãƒªã‚·ãƒ¼ or ä¼šè©±
    def followup_route(state: AgentState):
        if state.get("route_decision") == "conversational":
            return "generate_conversational"
        return "policy_gate"

    workflow.add_conditional_edges("followup_classifier", followup_route, {
        "generate_conversational": "generate_conversational",
        "policy_gate": "policy_gate",
    })

    # 4. ãƒãƒªã‚·ãƒ¼ã‚²ãƒ¼ãƒˆ -> æ¤œç´¢ or ä¼šè©±(æ‹’å¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)
    def pg_route(state: AgentState):
        # policy_gateå†…ã§æ‹’å¦(final_answerè¨­å®šæ¸ˆ)ãªã‚‰ä¼šè©±çµ‚äº†ã¸
        if state.get("final_answer"): 
            return "finalize_conversational" # ãã®ã¾ã¾çµ‚äº†ã¸
        
        rd = state.get("route_decision", "retrieval")
        if rd == "conversational":
            return "generate_conversational"
        return "retrieve"

    workflow.add_conditional_edges("policy_gate", pg_route, {
        "finalize_conversational": "finalize_conversational",
        "generate_conversational": "generate_conversational",
        "retrieve": "retrieve"
    })

    # 5. æ¤œç´¢ãƒ•ãƒ­ãƒ¼
    workflow.add_edge("retrieve", "generate_retrieval")
    workflow.add_edge("generate_retrieval", "grade_and_plan")
    
    def grade_route(state: AgentState):
        if state.get("sufficiency_decision") == "insufficient":
            return "check_ambiguity"
        return "fact_check"

    workflow.add_conditional_edges("grade_and_plan", grade_route, {
        "check_ambiguity": "check_ambiguity",
        "fact_check": "fact_check"
    })

    # 6. Ambiguity -> å†æ¤œç´¢ or çµ‚äº†
    def ambiguity_route(state: AgentState):
        if state.get("route_decision") == "ambiguous":
            return "finalize_conversational" # è«¦ã‚ã¦çµ‚äº†
        return "retrieve" # å†æ¤œç´¢

    workflow.add_conditional_edges("check_ambiguity", ambiguity_route, {
        "finalize_conversational": "finalize_conversational",
        "retrieve": "retrieve"
    })

    # 7. ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯å¾Œ
    def fact_route(state: AgentState):
        if state['fact_check_result'].get('is_grounded'):
            return "finalize_retrieval"
        return "rewrite_fact"

    workflow.add_conditional_edges("fact_check", fact_route, {
        "finalize_retrieval": "finalize_retrieval",
        "rewrite_fact": "rewrite_fact"
    })

    workflow.add_edge("rewrite_fact", "finalize_retrieval")
    workflow.add_edge("finalize_retrieval", END)
    
    workflow.add_edge("generate_conversational", "finalize_conversational")
    workflow.add_edge("finalize_conversational", END)

    return workflow.compile()


# ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ«å°¾ã«è¿½åŠ 
def build_graph(chatbot_instance: AdkChatbot):
    return build_support_agent_graph(chatbot_instance)