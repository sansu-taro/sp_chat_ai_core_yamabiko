from .chat_engine_adk_bq import AdkChatbot
from google import genai
from google.genai import types


from typing import TypedDict, Annotated, List, Sequence
import operator
import json
import time
from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ----------------------------------------------------------------
# 1. Stateã®å®šç¾© (ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‰Šé™¤ãƒ»æ•´ç†)
# ----------------------------------------------------------------
class AgentState(TypedDict):

    # â–¼â–¼â–¼ APPã‹ã‚‰ã®å—ã‘æ¸¡ã—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ â–¼â–¼â–¼
    conversation_id: str
    recognized_products: str
    
    # â–¼â–¼â–¼ ç®¡ç†ç”¨ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ â–¼â–¼â–¼
    session_id: str
    message_index: int
    user_question: str
    retrieved_context: str
    human_readable_context: str
    initial_answer: str
    fact_check_result: dict
    final_answer: str
    route_decision: str
    messages: Sequence[BaseMessage]

    # â–¼â–¼â–¼ æ¤œç´¢ãƒ«ãƒ¼ãƒ—ç®¡ç†ç”¨ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ â–¼â–¼â–¼
    current_query: str          # ç¾åœ¨ã®æ¤œç´¢ã‚¯ã‚¨ãƒª
    search_attempts: int        # æ¤œç´¢ã®è©¦è¡Œå›æ•°
    sufficiency_decision: str   # å›ç­”ãŒååˆ†ã‹ã©ã†ã‹ã®åˆ¤æ–­ ("sufficient" or "insufficient")

    # â–¼ å›³ã®ä¸‹å´ãƒ•ãƒ­ãƒ¼å¯¾å¿œç”¨
    conversation_phase: str      # "new" | "after_answer"
    followup_type: str           # "resolved" | "same_feature" | "different_feature" | "escalation"
    # â–¼â–¼â–¼ è¿½åŠ : ãƒŠãƒ¬ãƒƒã‚¸ä¸è¶³ãƒ•ãƒ©ã‚° â–¼â–¼â–¼
    is_knowledge_missing: bool      # Trueãªã‚‰ã€ŒãƒŠãƒ¬ãƒƒã‚¸0ä»¶ï¼†éå»ãƒ­ã‚°ã‚ã‚Šã€ã®çŠ¶æ…‹

    # â˜… è¿½åŠ : èãè¿”ã—ç™ºç”Ÿãƒ•ãƒ©ã‚°ï¼ˆãƒ­ã‚°è¨˜éŒ²ç”¨ï¼‰
    is_clarification_required: bool 
    # â˜… è¿½åŠ : é€šéã—ãŸãƒ«ãƒ¼ãƒˆã®å±¥æ­´è¨˜éŒ²ç”¨ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ»åˆ†æç”¨ï¼‰
    route_history: List[str]
    
    # â–¼â–¼â–¼ Plan-and-Executeç”¨ã«è¿½åŠ  â–¼â–¼â–¼
    plan_queue: list[str]       # æœªå®Ÿè¡Œã®æ¤œç´¢ã‚¯ã‚¨ãƒªãƒªã‚¹ãƒˆ
    completed_steps: list[str]  # å®Ÿè¡Œæ¸ˆã¿ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆãƒ­ã‚°ç”¨ï¼‰

    # â˜… è¿½åŠ : æœ€çµ‚ã‚¢ã‚¦ãƒˆã‚«ãƒ 
    final_outcome: str            # "answered" | "clarification" | "not_found" | "refused"
    final_outcome_reason: str     # ä»»æ„ï¼ˆãƒ­ã‚°åˆ†æç”¨ï¼‰
    # â˜… è¿½åŠ : æ™‚é–“è¨ˆæ¸¬ç”¨
    start_time: float       # å‡¦ç†é–‹å§‹æ™‚ã®UNIXã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    processing_time: float  # æœ€çµ‚çš„ã«ã‹ã‹ã£ãŸç§’æ•°

# ----------------------------------------------------------------
# 2. Agentã‚¯ãƒ©ã‚¹ï¼ˆã‚·ãƒ³ãƒ—ãƒ«åŒ–ï¼‰
# ----------------------------------------------------------------
class SupportOperationAgent:
    def __init__(self, chatbot: AdkChatbot):
        self.chatbot = chatbot
        self.client = getattr(chatbot, "gclient", None) or genai.Client()
        self.model_name = "gemini-2.5-flash"
        self.logger = logging.getLogger(__name__)
        
        # â˜… ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãªã©ã‚’å‰Šé™¤

    # --- å…±é€šãƒ˜ãƒ«ãƒ‘ ---
    def _gen(self, prompt: str, *, response_mime_type: str | None = None, temperature: float | None = 0.0):
        cfg = types.GenerateContentConfig(
            response_mime_type=response_mime_type,
            temperature=0.0 if temperature is None else temperature,
        )
        return self.client.models.generate_content(
            model=self.model_name,
            contents=prompt,
            config=cfg,
        )

    def _append_resolution_check(self, answer: str) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®æœ€çµ‚å›ç­”ã®æœ«å°¾ã«ã€è§£æ±ºç¢ºèªã®ä¸€æ–‡ã‚’ä»˜ä¸ã™ã‚‹ã€‚"""
        tail = (
            "\n\n"
            "â€•â€•â€•â€•\n"
            "è¿½åŠ ã®ã”è³ªå•ã®å ´åˆã¯ã€ç¶šã‘ã¦ã“ã¡ã‚‰ã§ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚\n"
            "ã“ã‚Œã¾ã§ã®ã”è³ªå•å†…å®¹ã‚’è¸ã¾ãˆã¦ã”æ¡ˆå†…ã„ãŸã—ã¾ã™ã€‚ï¼ˆâ€»å¯¾è±¡æ©Ÿèƒ½åã¯å†åº¦é¸æŠã„ãŸã ãå¿…è¦ãŒã”ã–ã„ã¾ã™ï¼‰\n"
            "æœ‰äººã‚µãƒãƒ¼ãƒˆã«åˆ‡ã‚Šæ›¿ãˆã‚‹å ´åˆã¯ã€Œè§£æ±ºã—ã¾ã—ãŸã‹ï¼Ÿã€ã®å›ç­”å¾Œã€è¿½åŠ ã§ã€Œè³ªå•ã™ã‚‹ã€> ã€Œã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ã¸æ¥ç¶šã€ã¸ãŠé€²ã¿ãã ã•ã„ã€‚"
        )
        return (answer or "").rstrip() + tail

    # ----------------------------------------------------------
    # å…¥å£ãƒ«ãƒ¼ã‚¿ãƒ¼
    # ----------------------------------------------------------
    def entry_router(self, state: AgentState):
        self.logger.info("---ğŸšª Node: entry_router ---")
        # é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ² (Stateã«ç„¡ã‘ã‚Œã°ç¾åœ¨æ™‚åˆ»)
        start_time = state.get("start_time") or time.time()
        messages = state.get("messages", [])
        has_ai_before = any(not isinstance(m, HumanMessage) for m in messages[:-1]) if messages else False
        
        if messages and isinstance(messages[-1], HumanMessage) and has_ai_before:
            phase = "after_answer"
        else:
            phase = "new"
        return {"conversation_phase": phase,
                "start_time": start_time
               }
        
    # Node: æ„å›³åˆ†é¡
    def classify_intent(self, state: AgentState):
        self.logger.info("---ğŸ§­ Node: classify_intent---")
        user_question = state['messages'][-1].content
            
        prompt = f"""
        æ¬¡ã®ç™ºè©±ã®æ„å›³ã‚’1ã¤ã ã‘é¸ã³ã€æ—¥æœ¬èªã®ã¾ã¾JSONã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        - "retrieval": è£½å“ä»•æ§˜/æ“ä½œ/æ–™é‡‘ãªã©ã®æƒ…å ±ç…§ä¼šï¼ˆãƒŠãƒ¬ãƒƒã‚¸å‚ç…§ã§å›ç­”å¯èƒ½ï¼‰
        - "conversational": æŒ¨æ‹¶ãƒ»é›‘è«‡ãƒ»ãŠç¤¼ãªã©ï¼ˆæ¤œç´¢ä¸è¦ï¼‰
        ç™ºè©±: {user_question}
        å‡ºåŠ›: {{"intent":"retrieval"|"conversational"}}
        """
        resp = self._gen(prompt, response_mime_type="application/json")
        
        try:
            data = json.loads(getattr(resp, "text", "") or "{}")
            intent = data.get("intent", "retrieval")
        except Exception:
            intent = "retrieval"
        
        route_decision = "retrieval" if intent == "retrieval" else "conversational"
        
        return {
            "user_question": user_question,
            "route_decision": route_decision,
            "current_query": user_question,
            "search_attempts": 0,
            "retrieved_context": "",
            "final_outcome": "",
            "final_outcome_reason": "",
        }


    # Node: ãƒãƒªã‚·ãƒ¼ã‚²ãƒ¼ãƒˆï¼ˆå†ã€…ã€…ä¿®æ­£ç‰ˆï¼šä»•æ§˜ç¢ºèªãƒ»ãƒ­ã‚¸ãƒƒã‚¯ç…§ä¼šè¨±å®¹ï¼‰
    def policy_gate(self, state: AgentState):
        self.logger.info("---ğŸš§ Node: policy_gate (Improved v4)---")
        target_q = state.get("current_query") or state["user_question"]

        prompt = f"""
        æ¬¡ã®è³ªå•ãŒãƒãƒªã‚·ãƒ¼ä¸Šã€AIãŒå›ç­”ã™ã¹ãã§ãªã„å†…å®¹ï¼ˆå€‹äººæƒ…å ±ç…§ä¼šã€å¥‘ç´„è©³ç´°ãªã©ï¼‰ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
        ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ŒSmartHRã€ã¨ã„ã†SaaSè£½å“ã®æ“ä½œã‚µãƒãƒ¼ãƒˆã§ã™ã€‚

        # åˆ¤å®šåŸºæº–
        ## ğŸš« å›ç­”ä¸å¯ (need_escalation: true)
        - ç‰¹å®šã®ä¼æ¥­ã®ã€Œå¥‘ç´„å†…å®¹ã€ã‚„ã€Œè«‹æ±‚é‡‘é¡ã€ã®ç¢ºèª
        - **AIã«å¯¾ã—ã¦ã€ç›´æ¥çš„ãªãƒ‡ãƒ¼ã‚¿æ“ä½œã‚„ç…§ä¼šã‚’ä¾é ¼ã™ã‚‹ã‚‚ã®** (ä¾‹:ã€ŒAã•ã‚“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä»Šã™ãç›´ã—ã¦ã€ã€Œç§ã®ä»£ã‚ã‚Šã«å‰Šé™¤ã—ã¦ã€)
          â€»ã€ŒAIãŒå®Ÿè¡Œã™ã‚‹ã€ã“ã¨ã¯ä¸å¯èƒ½ã§ã™ã€‚
        - **å€‹åˆ¥ã®æ³•çš„ãƒ»ç¨å‹™çš„ãªåˆ¤æ–­ï¼ˆã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ï¼‰**

        ## âœ… å›ç­”OK (need_escalation: false)
        - ä¸€èˆ¬çš„ãªæ©Ÿèƒ½ã®ä½¿ã„æ–¹ã€æ“ä½œæ‰‹é †ã€ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
        - **ã€Œæ©Ÿèƒ½ã®æœ‰ç„¡ã€ã‚„ã€Œæ“ä½œæ‰‹é †ã€ã«é–¢ã™ã‚‹è³ªå•**
          - ã€Œãƒ‡ãƒ¼ã‚¿ã‚’é–“é•ã£ã¦æ¶ˆã—ãŸã€å¾©æ—§ã§ãã‚‹ã‹ï¼Ÿã€ã¨ã„ã†è³ªå•ã¯ã€AIã¸ã®ä½œæ¥­ä¾é ¼ã§ã¯ãªãã€**ã€Œå¾©æ—§æ©Ÿèƒ½ã¯ã‚ã‚‹ã‹ï¼ˆä»•æ§˜ï¼‰ã€ã€Œã©ã†æ“ä½œã™ã‚Œã°æˆ»ã‚‹ã‹ï¼ˆæ‰‹é †ï¼‰ã€ã‚’èã„ã¦ã„ã‚‹**ã¨è§£é‡ˆã—ã€OKã¨ã—ã¦ãã ã•ã„ã€‚
          - ã€Œå‰Šé™¤ã®æ–¹æ³•ã€ã‚„ã€Œè¨‚æ­£ã®æ–¹æ³•ã€ã‚’èã‹ã‚ŒãŸå ´åˆã‚‚ã€æ‰‹é †ã®æ¡ˆå†…ã¨ã—ã¦OKã¨ã—ã¦ãã ã•ã„ã€‚
        - **ç”»é¢ä¸Šã®ç‰¹å®šã®è¡¨ç¤ºã‚„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå‡ºã‚‹ã€Œæ¡ä»¶ã€ã€ŒåŸå› ã€ã€Œä»•æ§˜ã€ã«é–¢ã™ã‚‹è³ªå•**
        - **ã‚·ã‚¹ãƒ†ãƒ æŒ™å‹•ã®æ­£å½“æ€§ç¢ºèª**

        è³ªå•: {target_q}
        å‡ºåŠ›å½¢å¼: {{"need_escalation": true/false, "reason": "ç†ç”±"}}
        """
        
        r = self._gen(prompt, response_mime_type="application/json")
        data = json.loads(getattr(r, "text", "") or "{}")
        is_ng = data.get("need_escalation", False)
        reason = data.get("reason", "")

        if is_ng:
            self.logger.info(f"    - Policy Blocked: {reason}")
            refusal_msg = (
                "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å€‹äººæƒ…å ±ã‚„å¥‘ç´„è©³ç´°ã«é–¢ã™ã‚‹ãŠå•ã„åˆã‚ã›ã€"
                "ã¾ãŸã¯å€‹åˆ¥ã®æ³•çš„ãƒ»ç¨å‹™çš„ãªåˆ¤æ–­ã‚’è¦ã™ã‚‹ã”è³ªå•ã«ã¯ã€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã¯ãŠç­”ãˆã§ãã¾ã›ã‚“ã€‚\n"
                "ãŠæ‰‹æ•°ã§ã™ãŒã€æ‹…å½“è€…ã¸ç›´æ¥ãŠå•ã„åˆã‚ã›ã„ãŸã ã‘ã¾ã™ã‚ˆã†ãŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚"
            )
            return {
                "route_decision": "conversational",
                "final_answer": refusal_msg,
                "final_outcome": "refused",
                "final_outcome_reason": reason or "policy_gate_blocked",
            }

        return {"route_decision": state.get("route_decision", "retrieval")}

    # ----------------------------------------------------------
    # â˜… NEW: Planning Node (æ¤œç´¢å‰ã®ã‚¿ã‚¹ã‚¯åˆ†è§£)
    # ----------------------------------------------------------
    def make_plan(self, state: AgentState):
        self.logger.info("---ğŸ“… Node: make_plan ---")
        q = state["user_question"]
        
        # è¤‡åˆçš„ãªè³ªå•ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹
        # â€» ã“ã“ã§ã€Œæ¤œç´¢ä¸è¦ã€ã¨åˆ¤æ–­ã•ã‚Œã‚Œã°ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã—ã¦ä¼šè©±ã¸ç›´è¡Œã•ã›ã‚‹åˆ¶å¾¡ã‚‚å¯èƒ½
        prompt = f"""
        ã‚ãªãŸã¯æ¤œç´¢ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã™ã‚‹ãŸã‚ã«å¿…è¦ãªã€Œæ¤œç´¢ã‚¯ã‚¨ãƒªã€ã‚’æ´—ã„å‡ºã—ã€å®Ÿè¡Œé †ã«ãƒªã‚¹ãƒˆåŒ–ã—ã¦ãã ã•ã„ã€‚
        
        # æ–¹é‡
        - å˜ç´”ãªè³ªå•ã§ã‚ã‚Œã°ã€ã‚¯ã‚¨ãƒªã¯1ã¤ã§ååˆ†ã§ã™ã€‚
        - è¤‡åˆçš„ãªè³ªå•ï¼ˆä¾‹ï¼šã€ŒAã®è¨­å®šæ–¹æ³•ã¨Bã®å‰Šé™¤æ–¹æ³•ã€ï¼‰ã®å ´åˆã€ãã‚Œãã‚Œã®ã‚¯ã‚¨ãƒªã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚
        - è³ªå•ãŒæŠ½è±¡çš„ãªå ´åˆã€å…·ä½“çš„ãªã„ãã¤ã‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆ†è§£ã—ã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚
        - **æœ€å¤§5ã‚¹ãƒ†ãƒƒãƒ—**ã¾ã§è¨­å®šå¯èƒ½ã§ã™ã€‚ç¶²ç¾…æ€§ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        {q}
        
        # å‡ºåŠ›å½¢å¼ (JSON)
        {{
            "queries": ["ã‚¯ã‚¨ãƒª1", "ã‚¯ã‚¨ãƒª2", ...]
        }}
        """
        
        resp = self._gen(prompt, response_mime_type="application/json", temperature=0.0)
        try:
            data = json.loads(getattr(resp, "text", "") or "{}")
            queries = data.get("queries", [])
        except:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®è³ªå•ã‚’ãã®ã¾ã¾1ã¤ã®ã‚¯ã‚¨ãƒªã¨ã—ã¦æ‰±ã†
            queries = [q]
            
        # ç©ºã®å ´åˆã¯æœ€ä½1ã¤å…¥ã‚Œã‚‹ï¼ˆæ¤œç´¢ãƒ«ãƒ¼ãƒˆã«æ¥ã¦ã„ã‚‹å‰æã®ãŸã‚ï¼‰
        if not queries:
            queries = [q]

        queries = queries[:5]
        self.logger.info(f"    - Plan created: {queries}")
        
        return {
            "plan_queue": queries,
            "completed_steps": []
        }

    # ----------------------------------------------------------
    # â˜… NEW: Execution Dispatcher (è¨ˆç”»å®Ÿè¡Œãƒ«ãƒ¼ã‚¿ãƒ¼)
    # ----------------------------------------------------------
    def execute_dispatch(self, state: AgentState):
        """
        plan_queue ã‹ã‚‰æ¬¡ã®ã‚¯ã‚¨ãƒªã‚’å–ã‚Šå‡ºã—ã€retrieve ã¸æ¸¡ã™æº–å‚™ã‚’ã™ã‚‹ã€‚
        ã‚­ãƒ¥ãƒ¼ãŒç©ºãªã‚‰ generate ã¸é€²ã‚€ãŸã‚ã®ãƒ•ãƒ©ã‚°ã‚’è¿”ã™ã€‚
        """
        queue = state.get("plan_queue", [])
        
        if not queue:
            # è¨ˆç”»å®Œäº† -> å›ç­”ç”Ÿæˆã¸
            return {"route_decision": "ready_to_answer"}
        
        # æ¬¡ã®ã‚¿ã‚¹ã‚¯ã‚’å–ã‚Šå‡ºã™
        next_query = queue[0]
        remaining_queue = queue[1:]
        
        self.logger.info(f"---ğŸš€ Dispatch: Next query -> '{next_query}' ---")
        
        return {
            "current_query": next_query,   # ã“ã‚ŒãŒ retrieve ãƒãƒ¼ãƒ‰ã§ä½¿ã‚ã‚Œã‚‹
            "plan_queue": remaining_queue, # ã‚­ãƒ¥ãƒ¼ã‚’æ›´æ–°
            "route_decision": "continue_search"
        }
        

    # ----------------------------------------------------------
    # Node: æƒ…å ±æ¤œç´¢ (æ”¹ä¿®: Append & Flag Merge)
    # ----------------------------------------------------------
    def retrieve(self, state: AgentState):
        self.logger.info("---ğŸ” Node: retrieve (Plan Execution)---")
        query = state['current_query']
        conversation_id = state.get('conversation_id')
        session_id =  state.get('session_id')
        message_index = state.get('message_index')

        # 1. æ¤œç´¢å®Ÿè¡Œ
        ai_context, human_context, search_meta = self.chatbot._get_information_for_query(
            query,
            conversation_id=conversation_id,
            session_id=session_id,
            message_index=message_index,
        )

        # 2. ãƒ•ãƒ©ã‚°ã®ORçµ±åˆ (éå»ã«Trueãªã‚‰ãšã£ã¨True)
        previous_missing = state.get("is_knowledge_missing", False)
        current_missing = search_meta.get("is_knowledge_missing", False)
        integrated_missing_flag = previous_missing or current_missing
        
        if current_missing:
            self.logger.warning(f"    âš ï¸ Query '{query}' hit NO official knowledge.")

        # 3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¿½è¨˜ (Append)
        existing_context = state.get('retrieved_context', '')
        # æ˜ç¤ºçš„ã«åŒºåˆ‡ã‚Šç·šã‚’å…¥ã‚Œã‚‹
        updated_context = (existing_context + f"\n\n--- æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã®çµæœ ---\n" + (ai_context or "")).strip()

        existing_human_context = state.get('human_readable_context', '')
        updated_human_context = (existing_human_context + "\n" + (human_context or "")).strip() if existing_human_context else (human_context or "")
        
        history = state.get("route_history", []) + ["retrieve"]

        return {
            "retrieved_context": updated_context,
            "human_readable_context": updated_human_context,
            "is_knowledge_missing": integrated_missing_flag,
            "route_history": history,
        }

    # Node: æ›–æ˜§æ€§ãƒã‚§ãƒƒã‚¯ (â˜…ã“ã“ã‚’ä¿®æ­£)
    def check_ambiguity(self, state: AgentState):
        self.logger.info("---âš–ï¸ Node: check_ambiguity---")
        history = state.get("route_history", []) + ["check_ambiguity"]

        attempts = state.get("search_attempts", 0)
        max_attempts = 3
        is_last_try = (attempts >= max_attempts)

        # ç›´å‰ã®AIå›ç­”ï¼ˆGradeã§Insufficientã¨ã•ã‚ŒãŸã‚‚ã®ï¼‰
        initial_answer = state.get("initial_answer", "")

        # â˜… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿®æ­£:
        # å˜ãªã‚‹æƒ…å ±ä¸è¶³(Clear)ãªã®ã‹ã€æ–‡è„ˆçš„ã«æ›–æ˜§ã§èãè¿”ã™å¿…è¦ãŒã‚ã‚‹(Ambiguous)ã®ã‹ã‚’å³å¯†ã«åˆ¤å®š
        # éå»ãƒ­ã‚°ç”±æ¥ã®ã€Œä¸é©åˆ‡ãªèãè¿”ã—ã€ã‚’é™¤å¤–ã™ã‚‹ãŸã‚ã€æ¤œç´¢çµæœ(retrieved_context)ã«ç…§ã‚‰ã—ã¦å¦¥å½“æ€§ã‚‚ãƒã‚§ãƒƒã‚¯
        prompt = f"""
        ç¾åœ¨ã®å›ç­”å€™è£œã¯ã€Œä¸ååˆ†ã€ã¨åˆ¤å®šã•ã‚Œã¾ã—ãŸã€‚
        ã“ã‚ŒãŒã€Œæ¤œç´¢ä¸è¶³ã€ã«ã‚ˆã‚‹ã‚‚ã®ã‹ã€è³ªå•ãŒã€Œæ›–æ˜§ã€ã§çµã‚Šè¾¼ã‚ãªã„ãŸã‚ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

        # åˆ¤æ–­ææ–™
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {state['user_question']}
        - ç¾åœ¨ã®æ¤œç´¢çµæœ: {state['retrieved_context'][:5000]}
        - ç”Ÿæˆã•ã‚ŒãŸå›ç­”å€™è£œ: {initial_answer}

        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ±ºå®š
        1. **Ambiguous (èãè¿”ã—ãŒå¿…è¦)**: 
           - æ¤œç´¢çµæœã«ã€ŒAã®å ´åˆã¯Xã€Bã®å ´åˆã¯Yã€ã¨ã„ã£ãŸåˆ†å²æƒ…å ±ãŒã‚ã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç¾çŠ¶ãŒä¸æ˜ãªãŸã‚å›ç­”ã‚’ä¸€æ„ã«çµã‚Œãªã„å ´åˆã€‚
           - **æ³¨æ„**: ç”Ÿæˆã•ã‚ŒãŸå›ç­”å€™è£œãŒã€Œèãè¿”ã—ã€ã‚’è¡Œã£ã¦ã„ã¦ã‚‚ã€ãã‚ŒãŒæ¤œç´¢çµæœã«åŸºã¥ã‹ãªã„ä¸é©åˆ‡ãªã‚‚ã®ï¼ˆä»–è£½å“ã®ä»•æ§˜ãªã©ï¼‰ã§ã‚ã‚‹å ´åˆã¯ã€ã“ã“ã‚’é¸ã°ãšã« "Clear" (å†æ¤œç´¢) ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚
           
        2. **Clear (å†æ¤œç´¢ãŒå¿…è¦)**: 
           - å˜ã«æƒ…å ±ãŒè¦‹ã¤ã‹ã£ã¦ã„ãªã„å ´åˆã€‚
           - ç”Ÿæˆã•ã‚ŒãŸå›ç­”å€™è£œã®ã€Œèãè¿”ã—ã€ãŒçš„å¤–ã‚Œãªå ´åˆã€‚

        # å‡ºåŠ› (JSON)
        {{
            "status": "ambiguous" | "clear",
            "clarification_question": "ambiguousã®å ´åˆã®ã¿ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ä¸å¯§ãªèãè¿”ã—æ–‡ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        }}
        """
        
        resp = self._gen(prompt, response_mime_type="application/json")
        data = json.loads(getattr(resp, "text", "") or "{}")
        status = data.get("status", "clear")

        if status == "ambiguous":
             self.logger.info("    - åˆ¤å®š: Ambiguous -> ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸èãè¿”ã—ã‚’å®Ÿè¡Œ")
             clarification_msg = data.get("clarification_question", "è©³ç´°ã‚’ãŠèã‹ã›ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ")
             
             return {
                 "route_decision": "ambiguous",
                 "final_answer": clarification_msg,
                 "is_clarification_required": True, # â˜… ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
                 "route_history": history,
                 "final_outcome": "clarification",
                 "final_outcome_reason": "need_more_context",
             }
        
        # Ambiguousã§ãªã„å ´åˆ
        if is_last_try:
             # ã‚‚ã†æ¤œç´¢å›æ•°ä¸Šé™ãªã‚‰è«¦ã‚ã‚‹
             self.logger.info("    - åˆ¤å®š: Clearã ãŒå›æ•°åˆ‡ã‚Œ -> çµ‚äº†")
             fallback_msg = (
                 "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ä½•åº¦ã‹æ¤œç´¢ã‚’è©¦ã¿ã¾ã—ãŸãŒã€"
                 "ã”è³ªå•ã«å¯¾ã™ã‚‹æ˜ç¢ºãªæƒ…å ±ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
             )
             return {
                 "route_decision": "ambiguous", # å¼·åˆ¶çµ‚äº†ãƒ«ãƒ¼ãƒˆã¸
                 "final_answer": fallback_msg,
                 "is_clarification_required": False,
                 "route_history": history,
                 "final_outcome": "not_found",
                 "final_outcome_reason": "max_attempts_reached",
             }
        
        # ã¾ã æ¤œç´¢ã§ãã‚‹ãªã‚‰å†æ¤œç´¢
        self.logger.info("    - åˆ¤å®š: Clear -> å†æ¤œç´¢ãƒ«ãƒ¼ãƒ—")
        return {
            "route_decision": "clear",
            "is_clarification_required": False,
            "route_history": history
        }

    # Node: å›ç­”ç”Ÿæˆ
    # Node 2: å›ç­”ç”Ÿæˆ
    def generate_initial_answer(self, state: AgentState):
        self.logger.info("---âœï¸ Node: generate_initial_answer (with retrieval)---")
        formatted_history = "".join(
            [f"ãŠå®¢æ§˜: {msg.content}\n" if isinstance(msg, HumanMessage) else f"AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {msg.content}\n"
             for msg in state['messages']]
        )

        prompt = f"""
        ã‚ãªãŸã¯ã€SmartHRã®ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒãƒ¼ãƒ ã«æ‰€å±ã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ã€ŒAIã€ã•ã‚“ã§ã™ã€‚
        æä¾›ã•ã‚ŒãŸã€Œæ ¹æ‹ æƒ…å ±ã€ã«åŸºã¥ã„ã¦ã€æ­£ç¢ºã‹ã¤è¦ªåˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

        # å›ç­”ç”Ÿæˆã®ãƒ«ãƒ¼ãƒ«
        1. **ã‚·ã‚¹ãƒ†ãƒ ç¢ºèªãƒ»å€‹åˆ¥èª¿æŸ»ã®ç¦æ­¢ã¨èª˜å°:**
           - æ ¹æ‹ æƒ…å ±ã®ä¸­ã«ã€Œã‚·ã‚¹ãƒ†ãƒ å´ã§ç¢ºèªã—ã¾ã™ã€ã€Œã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚„çµ„ç¹”å›³åã‚’æ•™ãˆã¦ãã ã•ã„ã€ã¨ã„ã£ãŸ**å€‹åˆ¥ã®èª¿æŸ»ã‚„ç¢ºèªã‚’ç”³ã—å‡ºã‚‹è¨˜è¿°**ãŒã‚ã‚‹å ´åˆã€**AIã§ã‚ã‚‹ã‚ãªãŸã¯ãã‚Œã‚’çµ¶å¯¾ã«å†ç¾ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚**
           - ä»£ã‚ã‚Šã«ã€**ã€Œã“ã®ä»¶ã¯ã‚·ã‚¹ãƒ†ãƒ çš„ãªç¢ºèªãŒå¿…è¦ã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æœ‰äººã‚µãƒãƒ¼ãƒˆï¼ˆã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼‰ã¸ç›´æ¥ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€** ã¨ã„ã†æ¡ˆå†…ã¸æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚
           - æ±ºã—ã¦ã€Œç§ï¼ˆAIï¼‰ãŒç¢ºèªã—ã¾ã™ã®ã§æƒ…å ±ã‚’æ•™ãˆã¦ãã ã•ã„ã€ã¨è¨€ã‚ãªã„ã§ãã ã•ã„ã€‚
        2. **æƒ…å ±ã®çµ±åˆ:** - ã€Œé–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ã€ã‚„ã€Œé¡ä¼¼éå»å›ç­”ã€ã‹ã‚‰ã€è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’æ¢ã—ã¦ãã ã•ã„ã€‚
           - å®Œå…¨ãªä¸€è‡´ï¼ˆã€Œã¯ã„ã€å¯èƒ½ã§ã™ã€ãªã©ï¼‰ãŒãªãã¦ã‚‚ã€**æ©Ÿèƒ½ã®ä»•æ§˜ã‚„æ“ä½œæ‰‹é †ã®è¨˜è¿°ã‹ã‚‰ã€è³ªå•ã«å¯¾ã™ã‚‹ç­”ãˆãŒè«–ç†çš„ã«å°ãå‡ºã›ã‚‹å ´åˆ**ã¯ã€ãã‚Œã‚’å›ç­”ã¨ã—ã¦æç¤ºã—ã¦ãã ã•ã„ã€‚
           - ä¾‹: è³ªå•ã€Œç®¡ç†è€…ã¯ç·¨é›†ã§ãã‚‹ã‹ï¼Ÿã€ã«å¯¾ã—ã€ãƒŠãƒ¬ãƒƒã‚¸ã«ã€Œç·¨é›†ç”»é¢ã‹ã‚‰æ›´æ–°ã§ãã¾ã™ã€ã¨ã‚ã‚Œã°ã€ã€Œã¯ã„ã€ç·¨é›†ç”»é¢ã‹ã‚‰æ›´æ–°å¯èƒ½ã§ã™ã€ã¨å›ç­”ã—ã¦æ§‹ã„ã¾ã›ã‚“ã€‚

        3. **æƒ…å ±æºã®å„ªå…ˆ:**
           - ã€Œé–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ã€ã®æƒ…å ±ã‚’æœ€å„ªå…ˆã—ã¦ãã ã•ã„ã€‚ã€Œé¡ä¼¼éå»å›ç­”ã€ã¯è£œè¶³ã¨ã—ã¦æ‰±ã„ã¾ã™ã€‚

        4. **æ¨æ¸¬ã®ç¯„å›²:**
           - æ ¹æ‹ æƒ…å ±ã«å…¨ãè¨˜è¿°ãŒãªã„æ©Ÿèƒ½ã‚„ä»•æ§˜ã«ã¤ã„ã¦ã¯ã€æ±ºã—ã¦å‰µä½œã—ãªã„ã§ãã ã•ã„ã€‚
           - ãŸã ã—ã€ä¸€èˆ¬çš„ãªæ“ä½œï¼ˆã€Œä¿å­˜ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã€ãªã©ï¼‰ã‚„ã€æ–‡è„ˆä¸Šæ˜ã‚‰ã‹ãªä¸»èªï¼ˆã€Œæ“ä½œç”»é¢ã€ã¨ã„ãˆã°é€šå¸¸ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼/ç®¡ç†è€…ãŒæ“ä½œã™ã‚‹ï¼‰ã«ã¤ã„ã¦ã¯ã€è£œã£ã¦èª¬æ˜ã—ã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚

        5. **æƒ…å ±ä¸è¶³ã®å ´åˆ:**
           - ä¸Šè¨˜ã‚’è¸ã¾ãˆã¦ã‚‚ç­”ãˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã¿ã€ã€Œæã‚Œå…¥ã‚Šã¾ã™ãŒã€ã„ãŸã ã„ãŸæƒ…å ±ã‹ã‚‰ã§ã¯æ˜ç¢ºãªã”æ¡ˆå†…ãŒé›£ã—ã„çŠ¶æ³ã§ã™ã€‚ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚

        # ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
        {formatted_history}
        # ãŠå®¢æ§˜ã®ç¾åœ¨ã®è³ªå•
        {state['user_question']}
        # æ ¹æ‹ æƒ…å ±
        {state['retrieved_context']}

        # å›ç­”:
        """
        response = self._gen(prompt, temperature=0.0)
        return {"initial_answer": getattr(response, "text", "")}

    # Node: ä¼šè©±ã®ã¿ã®å›ç­”
    def generate_conversational_answer(self, state: AgentState):
        # ã‚‚ã—policy_gateã§æ—¢ã«final_answerãŒè¨­å®šã•ã‚Œã¦ã„ãŸã‚‰ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—
        if state.get("final_answer"):
            return {}

        self.logger.info("---ğŸ’¬ Node: generate_conversational_answer---")
        
        formatted_history = "".join(
            [f"ãŠå®¢æ§˜: {msg.content}\n" if isinstance(msg, HumanMessage) else f"AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {msg.content}\n"
             for msg in state['messages']]
        )

        prompt = f"""
        ã‚ãªãŸã¯ã€SmartHRã®è¦ªåˆ‡ãªã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ŒAIã€ã•ã‚“ã§ã™ã€‚
        ã€Œã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã€ã‚’å‚è€ƒã«ã€ãŠå®¢æ§˜ã®ç¾åœ¨ã®è³ªå•ã«å¯¾ã—ã¦è‡ªç„¶ãªä¼šè©±ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚

        # æ³¨æ„äº‹é …
        - ã‚‚ã—ãŠå®¢æ§˜ãŒã€Œæ‹…å½“è€…ã«ã¤ãªã„ã§ã»ã—ã„ã€ã€Œé›»è©±ã—ãŸã„ã€ç­‰ã®è¦æœ›ã‚’å‡ºã—ã¦ã„ã‚‹å ´åˆã¯ã€
          ã€Œç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ç¾åœ¨ã¯AIã«ã‚ˆã‚‹è‡ªå‹•å¿œç­”ã®ã¿ã¨ãªã£ã¦ãŠã‚Šã¾ã™ã€‚ã“ã®ãƒãƒ£ãƒƒãƒˆã§è§£æ±ºã§ãã‚‹ã“ã¨ãŒã‚ã‚Œã°ãŠæ•™ãˆãã ã•ã„ã€
          ã¨ã„ã£ãŸè¶£æ—¨ã§ã€ä¸å¯§ã«ãŠæ–­ã‚Šã—ã¦ãã ã•ã„ã€‚

        # ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´
        {formatted_history}
        # ãŠå®¢æ§˜ã®ç¾åœ¨ã®è³ªå•
        {state['user_question']}
        # å¿œç­”
        """
        response = self._gen(prompt)
        return {"final_answer": getattr(response, "text", "")}


    # Node: è©•ä¾¡ãƒ»è¨ˆç”» (ä¿®æ­£ç‰ˆ)
    def grade_answer_and_plan(self, state: AgentState):
        self.logger.info("---ğŸ¤” Node: grade_answer_and_plan---")
        attempts = state.get("search_attempts", 0) + 1
        
        # ç°¡æ˜“å®Ÿè£…: å›ç­”ãŒç©ºãªã‚‰insufficient
        if not state.get("initial_answer"):
             # â˜…ä¿®æ­£: ã“ã“ã§ã‚‚å†æ¤œç´¢ã§ãã‚‹ã‚ˆã†ã«ã‚­ãƒ¥ãƒ¼ã«å…¥ã‚Œã‚‹ï¼ˆå…ƒã®è³ªå•ãªã©ï¼‰
             return {
                 "sufficiency_decision": "insufficient", 
                 "search_attempts": attempts,
                 "plan_queue": [state.get("user_question")] 
             }

        prompt = f"""
        ã‚ãªãŸã¯ã€ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å›ç­”ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹å“è³ªç®¡ç†è€…ã§ã™ã€‚
        ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€ã«å¯¾ã—ã¦ã€ã€Œç”Ÿæˆã•ã‚ŒãŸå›ç­”ã€ãŒè§£æ±ºç­–ã‚’æç¤ºã§ãã¦ã„ã‚‹ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

        # å…¥åŠ›æƒ…å ±
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {state['user_question']}
        - ç”Ÿæˆã•ã‚ŒãŸå›ç­”: {state['initial_answer']}

        # åˆ¤å®šåŸºæº–
        1. **Sufficient (ååˆ†)**:
           - è³ªå•ã«å¯¾ã™ã‚‹å…·ä½“çš„ãªæ‰‹é †ã€è§£æ±ºç­–ã€ã¾ãŸã¯Yes/NoãŒæç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚
           - å…¬å¼ãƒãƒ‹ãƒ¥ã‚¢ãƒ«(ãƒŠãƒ¬ãƒƒã‚¸)ãŒãªãã¦ã‚‚ã€éå»ã®å•ã„åˆã‚ã›å±¥æ­´(Past QA)ã‚’å¼•ç”¨ã—ã¦å›ç­”ã§ãã¦ã„ã‚‹å ´åˆã¯ã€Œååˆ†ã€ã¨ã™ã‚‹ã€‚
           - æ–‡è„ˆä¸Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®èãè¿”ã—ãŒå¿…è¦ã§ã€é©åˆ‡ãªè³ªå•ã‚’ã—ã¦ã„ã‚‹å ´åˆã‚‚ã€Œååˆ†ã€ã¨ã™ã‚‹ã€‚

        2. **Insufficient (ä¸è¶³)**:
           - ã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ã„ã†çµè«–ã®å ´åˆã€‚
           - è³ªå•ã¨å›ç­”ãŒã‹ã¿åˆã£ã¦ã„ãªã„å ´åˆã€‚

        # å‡ºåŠ› (JSONå½¢å¼ã®ã¿)
        {{
            "status": "sufficient" | "insufficient",
            "next_query": "insufficientã®å ´åˆã®ã¿ã€æ¬¡ã«æ¤œç´¢ã™ã¹ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"
        }}
        """
        resp = self._gen(prompt, response_mime_type="application/json")
        result = json.loads(getattr(resp, "text", "") or "{}")
        status = result.get("status", "sufficient")
        next_q = result.get("next_query", state["user_question"]) # fallbackã¯å…ƒã®è³ªå•

        if status == "insufficient":
            self.logger.info(f"    - åˆ¤å®š: Insufficient -> New Plan: {next_q}")
            return {
                "sufficiency_decision": "insufficient",
                "current_query": next_q,
                "plan_queue": [next_q], # â˜…é‡è¦: ã“ã“ã§ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€DispatchãŒretrieveã¸èª˜å°ã™ã‚‹
                "search_attempts": attempts
            }
        
        return {"sufficiency_decision": "sufficient", "search_attempts": attempts}

    
    # ----------------------------------------------------------
    # Node: ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ (URLãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¿½åŠ ç‰ˆ)
    # ----------------------------------------------------------
    def fact_check(self, state: AgentState):
        self.logger.info("---ğŸ”¬ Node: fact_check---")
        
        prompt = f"""
        ã‚ãªãŸã¯ã€ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®å›ç­”ã‚’ç›£æŸ»ã™ã‚‹ã€æ¥µã‚ã¦å³æ ¼ãªå“è³ªä¿è¨¼ï¼ˆQAï¼‰ã®å°‚é–€å®¶ã€ŒAIã€ã•ã‚“ã§ã™ã€‚
        ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã¯2ã¤ã‚ã‚Šã¾ã™ã€‚

        1.  **ç›£æŸ»:** ã€Œç”Ÿæˆã•ã‚ŒãŸå›ç­”ã€ãŒã€Œæ ¹æ‹ æƒ…å ±ã€ã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ã€ç‰¹ã«ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€ã®å‰æãŒèª¤ã£ã¦ã„ãªã„ã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
        2.  **æ¸…æ›¸ (ç›£æŸ»OKã®å ´åˆã®ã¿):** ã‚‚ã—ç›£æŸ»ã®çµæœãŒOK (is_grounded: true) ã ã£ãŸå ´åˆã€å›ç­”ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼æç¤ºç”¨ã®æœ€çµ‚å½¢å¼ã«æ¸…æ›¸ã—ã¾ã™ã€‚

        # è©•ä¾¡å¯¾è±¡
        - **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•**: {state['user_question']}
        - **æ ¹æ‹ æƒ…å ±**: {state['retrieved_context']}
        - **ç”Ÿæˆã•ã‚ŒãŸå›ç­” (æœ¬æ–‡ã®ã¿)**: {state['initial_answer']}

        # ç›£æŸ»åŸºæº– (æœ€å„ªå…ˆ)
        - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã®å‰æï¼ˆä¾‹ï¼šã€ŒAã®å¾Œã«Bã‚’ã™ã‚‹ã€ï¼‰ãŒã€æ ¹æ‹ æƒ…å ±ï¼ˆä¾‹ï¼šã€ŒBã®å¾Œã«Aã‚’ã™ã‚‹ã€ï¼‰ã¨çŸ›ç›¾ã—ã¦ã„ã‚‹å ´åˆã€å›ç­”ãŒãã®çŸ›ç›¾ã‚’æŒ‡æ‘˜ã›ãšå‰æã‚’è‚¯å®šã—ã¦ã„ã‚Œã°ã€**NG**ã§ã™ã€‚
        - å›ç­”ã«ã€æ ¹æ‹ æƒ…å ±ã«ãªã„æƒ…å ±ã‚„æ‹¡å¤§è§£é‡ˆãŒå«ã¾ã‚Œã¦ã„ã‚Œã°ã€**NG**ã§ã™ã€‚
        - å›ç­”å†…ã§ã€Œ**å¼Šç¤¾ã‚·ã‚¹ãƒ†ãƒ å´ã§ç¢ºèªã—ã¾ã™**ã€ã€Œ**ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æ•™ãˆã¦ãã ã•ã„**ã€ã€Œ**èª¿æŸ»ã—ã¾ã™**ã€ã¨ã„ã£ãŸã€AIã«ã¯å®Ÿè¡Œä¸å¯èƒ½ãªã‚·ã‚¹ãƒ†ãƒ èª¿æŸ»ã‚„å€‹äººæƒ…å ±ã®è´å–ã‚’è¡ŒãŠã†ã¨ã—ã¦ã„ãªã„ã‹ï¼Ÿã‚‚ã—å«ã¾ã‚Œã¦ã„ã‚Œã° **NG** ã§ã™ã€‚ã€Œã‚·ã‚¹ãƒ†ãƒ ç¢ºèªãŒå¿…è¦ãªå ´åˆã¯æœ‰äººã‚µãƒãƒ¼ãƒˆã¸èª˜å°ã™ã‚‹ã¹ãã€ã¨æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚
        
        # ã€é‡è¦ã€‘URLå¼•ç”¨ã¨ãƒªãƒ³ã‚¯ã®ãƒ«ãƒ¼ãƒ« (â˜…ã“ã“ã‚’å³å®ˆ)
        - å›ç­”å†…ã«ãƒªãƒ³ã‚¯ã‚’å«ã‚ã‚‹éš›ã¯ã€Markdownå½¢å¼ï¼ˆ[]()ï¼‰ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚
        - **ã€Œè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ« (URL)ã€** ã®å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
        - å›ç­”å†…ã«ãƒªãƒ³ã‚¯ã‚’åŸ‹ã‚è¾¼ã‚€éš›ã¯ã€å¿…ãš **`https://support.smarthr.jp/ja/help/articles` ã§å§‹ã¾ã‚‹å…¬å¼ãƒ˜ãƒ«ãƒ—ãƒšãƒ¼ã‚¸ã®URLã®ã¿** ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
        - `https://app.intercom.com` ã‚„ãã®ä»–ã®URLåŠã³ã‚¿ã‚¤ãƒˆãƒ«ã¯ã€ç¤¾å†…ç”¨ã¾ãŸã¯é¡§å®¢é–²è¦§ä¸å¯ã®ãŸã‚ã€**çµ¶å¯¾ã«å¼•ç”¨ãƒ»ãƒªãƒ³ã‚¯ã—ãªã„ã§ãã ã•ã„**ã€‚
        - æ ¹æ‹ ãŒã€Œéå»ã®å›ç­”(Past QA)ã€ã—ã‹ãªã„å ´åˆã¯ã€å†…å®¹ã¯å‚è€ƒã«ã—ã¦å›ç­”ã‚’ä½œæˆã—ã€**ãƒªãƒ³ã‚¯ã¯è²¼ã‚‰ãªã„ã§ãã ã•ã„**ã€‚
        - æ–‡å­—ã®å¼·èª¿ï¼ˆå¤ªå­— **text** ç­‰ï¼‰ã‚„è¦‹å‡ºã—ï¼ˆ#ï¼‰ãªã©ã®Markdownè¨˜æ³•ã¯ä¸€åˆ‡ä½¿ç”¨ã›ãšã€**ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        
        # å‡ºåŠ›å½¢å¼ (JSON)
        ç›£æŸ»ã®çµæœã€ä»¥ä¸‹ã®ã©ã¡ã‚‰ã‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

        ## 1. ç›£æŸ»ãŒOKã ã£ãŸå ´åˆ
        {{
            "is_grounded": true,
            "reason": "å›ç­”ã¯æ ¹æ‹ æƒ…å ±ã«åŸºã¥ã„ã¦ãŠã‚Šã€URLã®è¦å®šã‚‚å®ˆã‚‰ã‚Œã¦ã„ã¾ã™ã€‚",
            "formatted_answer": "ï¼ˆã“ã“ã«ã€ãƒ«ãƒ¼ãƒ«ã«å‰‡ã£ã¦æ¸…æ›¸æ¸ˆã¿ã®æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ï¼‰"
        }}
        
        ## 2. ç›£æŸ»ãŒNGã ã£ãŸå ´åˆ
        {{
            "is_grounded": false,
            "reason": "ï¼ˆã“ã“ã«ã€NGã¨åˆ¤æ–­ã—ãŸå…·ä½“çš„ãªç†ç”±ã‚’è¨˜è¿°ã™ã‚‹ã€‚ä¾‹ï¼šç¤¾å†…ç”¨URLãŒå«ã¾ã‚Œã¦ã„ã‚‹ã€æ ¹æ‹ ã¨çŸ›ç›¾ã™ã‚‹...ï¼‰",
            "formatted_answer": null
        }}

        # ã‚ãªãŸã®å‡ºåŠ› (JSONå½¢å¼ã®ã¿):
        """
        resp = self._gen(prompt, response_mime_type="application/json")
        data = json.loads(getattr(resp, "text", "") or "{}")
        
        if data.get("is_grounded"):
            return {"fact_check_result": data, "initial_answer": data.get("formatted_answer", "")}
        else:
            return {"fact_check_result": data}

    # ----------------------------------------------------------
    # Node: rewrite_answer (URLãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¿½åŠ ç‰ˆ)
    # ----------------------------------------------------------
    def rewrite_answer(self, state: AgentState):
        self.logger.info("---ğŸ”§ Node: rewrite_answer (fact check)---")
        reason = (state.get('fact_check_result') or {}).get('reason', '')
        
        prompt = f"""
        ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã§æŒ‡æ‘˜ã‚’å—ã‘ã¾ã—ãŸã€‚æŒ‡æ‘˜å†…å®¹ã‚’è¸ã¾ãˆã€å¿…ãšã€Œæ ¹æ‹ æƒ…å ±ã€ã®ã¿ã§å›ç­”ã‚’**ä¿®æ­£**ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æç¤ºã™ã‚‹æœ€çµ‚å½¢å¼ã«æ¸…æ›¸ã—ã¦ãã ã•ã„ã€‚

        # ã€é‡è¦ã€‘å½¢å¼ã®ãƒ«ãƒ¼ãƒ« (â˜…ã“ã“ã‚’å³å®ˆ)
        1. **Markdownç¦æ­¢:** å¤ªå­—ã€æ–œä½“ã€è¦‹å‡ºã—ãªã©ã®Markdownè¨˜æ³•ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚
        2. **URLã®è¨˜è¿°:** `[ãƒ†ã‚­ã‚¹ãƒˆ](URL)` ã§ã¯ãªãã€**`ãƒ†ã‚­ã‚¹ãƒˆ (URL)`** ã®å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
        3. **è¨±å¯ã•ã‚Œã‚‹URL:** `https://support.smarthr.jp/ja/help/articles` ã§å§‹ã¾ã‚‹URLã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚

        # æ§‹æˆæŒ‡ç¤º
        1. ä¿®æ­£å¾Œã®å›ç­”æœ¬æ–‡ï¼ˆå…¬å¼ãƒ˜ãƒ«ãƒ—ã¸ã®ãƒªãƒ³ã‚¯ã®ã¿åŸ‹ã‚è¾¼ã¿å¯ï¼‰

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {state['user_question']}
        # æ ¹æ‹ æƒ…å ±: {state['retrieved_context']}
        # åˆæœŸã®å›ç­”: {state['initial_answer']}
        # æŒ‡æ‘˜å†…å®¹: {reason}
        
        # ä¿®æ­£å¾Œã®å›ç­”:
        """
        resp = self._gen(prompt)
        return {"initial_answer": getattr(resp, "text", "")}

    # Node: æœ€çµ‚åŒ– (Retrieval)
    def finalize_retrieval_response(self, state: AgentState):
        self.logger.info("---ğŸ Node: finalize_retrieval_response---")
        # â˜… æ™‚é–“è¨ˆç®—
        start_ts = state.get("start_time", time.time())
        duration = time.time() - start_ts
        
        base = state["initial_answer"]
        final = self._append_resolution_check(base)
        return {"final_answer": final,
                "final_outcome": state.get("final_outcome") or "answered",
                "final_outcome_reason": state.get("final_outcome_reason") or "",
                "processing_time": duration,  # â˜… è¨ˆç®—çµæœã‚’Stateã¸
               }

    # Node: æœ€çµ‚åŒ– (Conversational)
    def finalize_conversational_response(self, state: AgentState):
        self.logger.info("---ğŸ Node: finalize_conversational_response---")
        # â˜… æ™‚é–“è¨ˆç®—
        start_ts = state.get("start_time", time.time())
        duration = time.time() - start_ts
        
        return {"final_answer": state['final_answer'],
                "final_outcome": state.get("final_outcome") or "answered",
                "final_outcome_reason": state.get("final_outcome_reason") or "",
                "processing_time": duration,
               }

    # Node: ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—åˆ†é¡
    def followup_classifier(self, state: AgentState):
        self.logger.info("---ğŸ” Node: followup_classifier ---")
        last_user = state["messages"][-1].content
        
        # ç›´å‰ã®ã‚¯ã‚¨ãƒªã‚’å–å¾—ï¼ˆæ–‡è„ˆç¶­æŒã®ãŸã‚ï¼‰
        previous_query = state.get("current_query", "")

        prompt = f"""
        æ¬¡ã®ãŠå®¢æ§˜ã®ç™ºè©±ã‚¿ã‚¤ãƒ—ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚
        - "resolved": è§£æ±ºå ±å‘Šãƒ»ãŠç¤¼
        - "followup": è¿½åŠ è³ªå•ï¼ˆåŒã˜è©±é¡Œã®æ·±æ˜ã‚Šï¼‰
        - "new_topic": å…¨ãåˆ¥ã®è©±é¡Œã¸ã®è»¢æ›
        - "escalation": æ‹…å½“è€…ã¸ç¹‹ã„ã§ã»ã—ã„ã¨ã„ã†è¦æœ›
        ç™ºè©±: {last_user}
        å‡ºåŠ›: {{"decision": "resolved"|"followup"|"new_topic"|"escalation"}}
        """
        r = self._gen(prompt, response_mime_type="application/json")
        data = json.loads(getattr(r, "text", "") or "{}")
        decision = data.get("decision", "followup")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ãã®ã¾ã¾ä½¿ã†
        next_query = last_user

        if decision == "resolved":
            route = "conversational"
        elif decision == "escalation":
            # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¸Œæœ›æ™‚ã¯ä¼šè©±ãƒ«ãƒ¼ãƒˆã¸ï¼ˆAIãŒæ–­ã‚Šã‚’å…¥ã‚Œã‚‹ï¼‰
            route = "conversational"
        elif decision == "followup":
            # â˜…é‡è¦ä¿®æ­£: åŒã˜è©±é¡Œã®æ·±æ˜ã‚Šãªã‚‰ã€å‰å›ã®ã‚¯ã‚¨ãƒªã¨çµåˆã—ã¦æ¤œç´¢ç²¾åº¦ã‚’ä¿ã¤
            # ä¾‹: "å¾“æ¥­å“¡ç™»éŒ²" + "ä¸€æ‹¬ã§ã§ãã‚‹ï¼Ÿ"
            route = "retrieval"
            if previous_query:
                next_query = f"{previous_query} {last_user}"
        else: # new_topic
            # è©±é¡Œè»¢æ›ãªã‚‰ã€æ–°ã—ã„ç™ºè¨€ã ã‘ã§æ¤œç´¢ã™ã‚‹
            route = "retrieval"
            next_query = last_user
            
        return {
            "route_decision": route,
            "user_question": last_user,
            "current_query": next_query # æ–‡è„ˆè€ƒæ…®æ¸ˆã¿ã®ã‚¯ã‚¨ãƒª
        }


# ----------------------------------------------------------------
# 3. ã‚°ãƒ©ãƒ•æ§‹ç¯‰ (escalate_to_human ãƒãƒ¼ãƒ‰å‰Šé™¤ç‰ˆ)
# ----------------------------------------------------------------

def build_support_agent_graph(chatbot_instance: AdkChatbot):
    agent = SupportOperationAgent(chatbot_instance)
    workflow = StateGraph(AgentState)

    # === ãƒãƒ¼ãƒ‰ç™»éŒ² ===
    # æ—¢å­˜ãƒãƒ¼ãƒ‰
    workflow.add_node("entry_router", agent.entry_router)
    workflow.add_node("classify_intent", agent.classify_intent)
    workflow.add_node("followup_classifier", agent.followup_classifier)
    workflow.add_node("policy_gate", agent.policy_gate)
    
    # â˜…è¿½åŠ : Plan & Execute ãƒãƒ¼ãƒ‰
    workflow.add_node("make_plan", agent.make_plan)
    workflow.add_node("execute_dispatch", agent.execute_dispatch)

    # æ¤œç´¢ãƒ»ç”Ÿæˆç³»ãƒãƒ¼ãƒ‰
    workflow.add_node("retrieve", agent.retrieve)
    workflow.add_node("generate_retrieval", agent.generate_initial_answer)
    workflow.add_node("grade_and_plan", agent.grade_answer_and_plan)
    workflow.add_node("check_ambiguity", agent.check_ambiguity) 
    workflow.add_node("fact_check", agent.fact_check)
    workflow.add_node("rewrite_fact", agent.rewrite_answer)
    workflow.add_node("finalize_retrieval", agent.finalize_retrieval_response)
    
    workflow.add_node("generate_conversational", agent.generate_conversational_answer)
    workflow.add_node("finalize_conversational", agent.finalize_conversational_response)

    # === ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ ===
    workflow.set_entry_point("entry_router")

    # 1. å…¥å£
    def entry_route(state: AgentState):
        return "after_answer" if state.get("conversation_phase") == "after_answer" else "new"

    workflow.add_conditional_edges("entry_router", entry_route, {
        "new": "classify_intent",
        "after_answer": "followup_classifier",
    })

    # 2. æ„å›³åˆ†é¡ -> ãƒãƒªã‚·ãƒ¼
    workflow.add_edge("classify_intent", "policy_gate")

    # 3. ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ— -> ãƒãƒªã‚·ãƒ¼ or ä¼šè©±
    def followup_route(state: AgentState):
        if state.get("route_decision") == "conversational":
            return "generate_conversational"
        return "policy_gate"

    workflow.add_conditional_edges("followup_classifier", followup_route, {
        "generate_conversational": "generate_conversational",
        "policy_gate": "policy_gate",
    })

    # 4. ãƒãƒªã‚·ãƒ¼ã‚²ãƒ¼ãƒˆ -> ã€ä¿®æ­£ã€‘æ¤œç´¢ãŒå¿…è¦ãªã‚‰ã€Œmake_planã€ã¸
    def pg_route(state: AgentState):
        if state.get("final_answer"): 
            return "finalize_conversational"
        
        rd = state.get("route_decision", "retrieval")
        if rd == "conversational":
            return "generate_conversational"
        
        # â˜…ã“ã“ãŒé‡è¦: ã„ããªã‚Š retrieve ã›ãšã€ã¾ãšã¯è¨ˆç”»(Plan)ã¸
        return "make_plan"

    workflow.add_conditional_edges("policy_gate", pg_route, {
        "finalize_conversational": "finalize_conversational",
        "generate_conversational": "generate_conversational",
        "make_plan": "make_plan" 
    })

    # 5. Planning -> Dispatch (è¨ˆç”»ã—ãŸã‚‰å®Ÿè¡Œç®¡ç†ã¸)
    workflow.add_edge("make_plan", "execute_dispatch")

    # 6. Dispatch ãƒ«ãƒ¼ãƒ— (å®Ÿè¡Œç®¡ç†ã«ã‚ˆã‚‹æŒ¯ã‚Šåˆ†ã‘)
    def dispatch_route(state: AgentState):
        # ã¾ã ã‚­ãƒ¥ãƒ¼ã«ã‚¯ã‚¨ãƒªãŒæ®‹ã£ã¦ã„ã‚Œã°æ¤œç´¢ã¸ã€ãªã‘ã‚Œã°å›ç­”ç”Ÿæˆã¸
        if state.get("route_decision") == "continue_search":
            return "retrieve"
        return "generate_retrieval"

    workflow.add_conditional_edges("execute_dispatch", dispatch_route, {
        "retrieve": "retrieve",
        "generate_retrieval": "generate_retrieval"
    })

    # â˜…é‡è¦: æ¤œç´¢ãŒçµ‚ã‚ã£ãŸã‚‰ Dispatch ã«æˆ»ã‚‹ (æ¬¡ã®ã‚¯ã‚¨ãƒªãŒã‚ã‚‹ã‹ç¢ºèªã™ã‚‹ãŸã‚)
    workflow.add_edge("retrieve", "execute_dispatch")

    # 7. å›ç­”ç”Ÿæˆ -> è©•ä¾¡(Grade)
    workflow.add_edge("generate_retrieval", "grade_and_plan")
    
    # 8. è©•ä¾¡çµæœã«ã‚ˆã‚‹åˆ†å² (ã“ã“ã§ã®å†æ¤œç´¢ã¯ã€Planå¤–ã®è£œæ­£ãªã®ã§ retrieve ã¸ç›´æ¥æˆ»ã—ã¦ã‚‚è‰¯ã„ãŒã€
    #    ä»Šå›ã¯ grade_and_plan ã§ plan_queue ã«è¿½åŠ ã™ã‚‹å®Ÿè£…ã«ã—ãŸãŸã‚ dispatch ã¸æˆ»ã™)
    def grade_route(state: AgentState):
        if state.get("sufficiency_decision") == "insufficient":
            return "check_ambiguity"
        return "fact_check"

    workflow.add_conditional_edges("grade_and_plan", grade_route, {
        "check_ambiguity": "check_ambiguity",
        "fact_check": "fact_check"
    })

    # 9. Ambiguity -> å†æ¤œç´¢(DispatchçµŒç”±) or çµ‚äº†
    def ambiguity_route(state: AgentState):
        if state.get("route_decision") == "ambiguous":
            return "finalize_conversational" 
        
        # ã‚¯ãƒªã‚¢(å†æ¤œç´¢)ã®å ´åˆã¯ã€Dispatchã¸æˆ»ã—ã¦å†æ¤œç´¢ã‚’å®Ÿè¡Œ
        return "execute_dispatch"

    workflow.add_conditional_edges("check_ambiguity", ambiguity_route, {
        "finalize_conversational": "finalize_conversational",
        "execute_dispatch": "execute_dispatch"
    })

    # 10. ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯å¾Œ
    def fact_route(state: AgentState):
        if state['fact_check_result'].get('is_grounded'):
            return "finalize_retrieval"
        return "rewrite_fact"

    workflow.add_conditional_edges("fact_check", fact_route, {
        "finalize_retrieval": "finalize_retrieval",
        "rewrite_fact": "rewrite_fact"
    })

    workflow.add_edge("rewrite_fact", "finalize_retrieval")
    workflow.add_edge("finalize_retrieval", END)
    
    workflow.add_edge("generate_conversational", "finalize_conversational")
    workflow.add_edge("finalize_conversational", END)

    return workflow.compile()


# ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ«å°¾ã«è¿½åŠ 
def build_graph(chatbot_instance: AdkChatbot):
    return build_support_agent_graph(chatbot_instance)